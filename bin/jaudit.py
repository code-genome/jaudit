#!/usr/bin/python
#
# (C) Copyright IBM 2023.
#
# This code is licensed under the Apache License, Version 2.0. You may
# obtain a copy of this license in the LICENSE.txt file in the root directory
# of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
#
# Any modifications or derivative works of this code must retain this
# copyright notice, and modified files need to carry a notice indicating
# that they have been altered from the originals.
#
#------------------------------------------------------------------------
#
# This file is autogenerated; if you are editing this file
# you are on the wrong path.
#
buildDate="2023/11/16"
gitRevision="364b818bb5594d969b0f194a65ea7a634c4ce011"

import sys
import os

import re
import json
import csv

import string

import subprocess
import struct
import argparse
import traceback
import inspect

import zipfile as ZipFile
import gzip as gzip

from hashlib import sha256
from socket import gethostname
from io import IOBase, BytesIO, TextIOWrapper

# global.py 

errorOut = None

class State:
    verbose=False
    scanTarFiles=False
    scanZipFiles=False
    addEvidence=True

    currentJar=[]

    analyticSet=[]

    analyticNames=set()
    enabledAnalytics=set()
    looseClasses=[]
# util.py 

#
# Python2 doesn't have bytestrings with a hex() method, so we do it
# the hard way.
#

def hexnyb(n):
    return "0123456789abcdef"[n]

def hexbyte(b):
    a = (b >> 4) & 15
    b = b & 15
    return hexnyb(a) + hexnyb(b)

def hex(data):
    res=[]
    for b in struct.unpack(str(len(data))+"B", data):
        res.append(hexbyte(b))
    return "".join(res)

def getplugins(parent):
    result = []
    gset = globals().items()
    for name, object in gset:
        if inspect.isclass(object):
            if issubclass(object, parent):
                if parent == object:
                    continue
                result.append(object)
    return result
# configuration.py 
#
# This is an generated file.  Do not edit.
#
class ConfigurationData:
    analytic_data = {"jar-digest":{"analytic":"jar-digest","size":5,"identifiers":{"dc22a":[[0,"-2.11.2"]],"e5be2":[[0,"-2.3.1"]],"c9b50":[[1,"-1.2.6"]],"9731b":[[1,"-1.2.9"]],"1ff2d":[[0,"-2.0-rc1"]],"0d654":[[0,"-2.0-beta3"]],"bfdcb":[[1,"-1.2.2"]],"eaa4c":[[0,"-2.11.1"]],"0c2cb":[[0,"-2.6"]],"c7d07":[[1,"-1.2.12"]],"4f019":[[0,"-2.0-beta9"]],"d83b8":[[0,"-2.9.0"]],"12adf":[[1,"-1.2.17"]],"6a5d3":[[0,"-2.8.2"]],"b5c14":[[0,"-2.17.0"]],"5f97c":[[0,"-2.0-beta5"]],"72d0d":[[0,"-2.17.1"]],"be2e0":[[0,"-2.12.2"]],"80cee":[[1,"-1.2.5"]],"d7ec8":[[1,"-1.2.14"]],"6a010":[[0,"-2.0-beta2"]],"9c39d":[[0,"-2.15.0"]],"fd572":[[0,"-2.13.3"]],"77e65":[[1,"-1.2.1"]],"99164":[[0,"-2.0-rc2"]],"8e75a":[[0,"-3.0.0-alpha1"]],"f1f26":[[0,"-2.6.1"]],"3586c":[[0,"-2.0-beta8"]],"b0855":[[1,"-1.3alpha-8"]],"a9368":[[1,"-1.0.4"]],"6bd98":[[0,"-2.0-alpha2"]],"f01d6":[[0,"-2.20.0"]],"42e01":[[0,"-2.3.2"]],"cfbc2":[[0,"-2.18.0"]],"41e3f":[[1,"-1.2.3"]],"2789c":[[0,"-2.14.0"]],"a341f":[[0,"-2.3"]],"6956e":[[0,"-2.13.1"]],"e0028":[[0,"-2.4"]],"06ce8":[[0,"-2.12.1"]],"a3117":[[1,"-1.2rc1"]],"1c3f6":[[0,"-2.11.0"]],"756ea":[[0,"-2.7"]],"c3033":[[0,"-2.0-beta1"]],"dcab6":[[0,"-2.13.2"]],"7cb8c":[[0,"-2.8.1"]],"7e96e":[[0,"-2.1"]],"c01b2":[[0,"-2.0"]],"5816a":[[1,"-1.2.11"]],"c739d":[[0,"-2.12.0"]],"b7e13":[[0,"-2.16.0"]],"0e955":[[0,"-2.0-alpha1"]],"e5630":[[0,"-2.14.1"]],"368a7":[[0,"-2.9.1"]],"c3a90":[[1,"-1.3alpha-5"]],"75218":[[1,"-1.2.15"]],"7c118":[[0,"-2.0-beta4"]],"f52e1":[[0,"-2.17.2"]],"e94ab":[[1,"-1.2.4"]],"cdabd":[[0,"-2.19.0"]],"80a07":[[0,"-2.0.1"]],"36c9a":[[1,"-1.2.7"]],"5f2ef":[[1,"-1.3alpha-6"]],"02fc1":[[1,"-1.3alpha-3"]],"9aedd":[[0,"-2.12.3"]],"ff084":[[1,"-1.2.16"]],"fb9b8":[[0,"-2.0-beta7"]],"4fd41":[[0,"-2.4.1"]],"60fd2":[[0,"-2.0-beta6"]],"b7dc0":[[0,"-2.12.4"]],"635ac":[[0,"-2.5"]],"6836b":[[0,"-2.10.0"]],"a7105":[[1,"-1.3alpha-7"]],"d9181":[[1,"-1.2beta4"]],"7735e":[[0,"-2.13.0"]],"85fd1":[[1,"-1.2.8"]],"8a09d":[[1,"-1.3alpha-1"]],"fd1a5":[[0,"-2.0.2"]],"2808c":[[1,"-1.2.13"]],"e6828":[[0,"-2.2"]],"73dc9":[[0,"-2.6.2"]],"bde5f":[[0,"-2.8"]]},"prefix-map":["log4j-core","log4j"]},"jar-fingerprint":{"analytic":"jar-fingerprint","size":5,"identifiers":{"fb9a3":[[0,"-2.11.2"]],"bc51e":[[0,"-2.3.1"]],"22c93":[[1,"-1.2.6"]],"d7504":[[1,"-1.2.9"]],"8eebc":[[0,"-2.0-rc1"]],"e6617":[[0,"-2.0-beta3"]],"eb3f3":[[1,"-1.2.2"]],"cd204":[[0,"-2.11.1"]],"10f00":[[0,"-2.6"]],"4b425":[[1,"-1.2.12"]],"6925a":[[0,"-2.0-beta9"]],"fefbb":[[0,"-2.9.0"]],"87942":[[1,"-1.2.17"]],"a9908":[[0,"-2.8.2"]],"458e7":[[0,"-2.17.0"]],"8f4bd":[[0,"-2.0-beta5"]],"1aaa8":[[0,"-2.17.1"]],"9d124":[[0,"-2.12.2"]],"8d196":[[1,"-1.2.5"]],"d255e":[[1,"-1.2.14"]],"46cc0":[[0,"-2.0-beta2"]],"12a85":[[0,"-2.15.0"]],"4ca8f":[[0,"-2.13.3"],[0,"-2.13.2"]],"1ba5e":[[1,"-1.2.1"]],"08598":[[0,"-2.0-rc2"]],"d977d":[[0,"-3.0.0-alpha1"]],"c5bfa":[[0,"-2.6.1"]],"24288":[[0,"-2.0-beta8"]],"a22f4":[[1,"-1.3alpha-8"]],"13552":[[1,"-1.0.4"]],"b9c5b":[[0,"-2.0-alpha2"]],"f9ca8":[[0,"-2.20.0"]],"89d2e":[[0,"-2.3.2"]],"54bd1":[[0,"-2.18.0"]],"53c3f":[[1,"-1.2.3"]],"fe00d":[[0,"-2.14.0"]],"b396a":[[0,"-2.3"]],"64006":[[0,"-2.13.1"]],"a9b6e":[[0,"-2.4"]],"4bf71":[[0,"-2.12.1"]],"108d2":[[1,"-1.2rc1"]],"a557b":[[0,"-2.11.0"]],"09df6":[[0,"-2.7"]],"55d5b":[[0,"-2.0-beta1"]],"26905":[[0,"-2.8.1"]],"77452":[[0,"-2.1"]],"b298f":[[0,"-2.0"]],"0c2fa":[[1,"-1.2.11"]],"aab11":[[0,"-2.12.0"]],"0cab9":[[0,"-2.16.0"]],"15b80":[[0,"-2.0-alpha1"]],"d7c56":[[0,"-2.14.1"]],"58387":[[0,"-2.9.1"]],"14635":[[1,"-1.3alpha-5"]],"3895b":[[1,"-1.2.15"]],"374fc":[[0,"-2.0-beta4"]],"ad0e3":[[0,"-2.17.2"]],"88e95":[[1,"-1.2.4"]],"fee69":[[0,"-2.19.0"]],"93f9c":[[0,"-2.0.1"]],"05db4":[[1,"-1.2.7"]],"e85bf":[[1,"-1.3alpha-6"]],"6bf11":[[1,"-1.3alpha-3"]],"71539":[[0,"-2.12.3"]],"3cf77":[[1,"-1.2.16"]],"9fa25":[[0,"-2.0-beta7"]],"2b7e0":[[0,"-2.4.1"]],"920ee":[[0,"-2.0-beta6"]],"6c51f":[[0,"-2.12.4"]],"07109":[[0,"-2.5"]],"8d879":[[0,"-2.10.0"]],"b7da7":[[1,"-1.3alpha-7"]],"7a333":[[1,"-1.2beta4"]],"d6614":[[0,"-2.13.0"]],"08b18":[[1,"-1.2.8"]],"98616":[[1,"-1.3alpha-1"]],"a51d4":[[0,"-2.0.2"]],"7696e":[[1,"-1.2.13"]],"6571a":[[0,"-2.2"]],"c2405":[[0,"-2.6.2"]],"d95b3":[[0,"-2.8"]]},"prefix-map":["log4j-core","log4j"]},"jar-name":{"analytic":"jar-name","identifiers":[{"regex":"^(log4j-core-[0-9][0-9\\.]*)\\.jar$"},{"regex":"^(log4j-core-[0-9][0-9\\.]*-?rc-?[0-9]*)\\.jar$"},{"regex":"^(log4j-core-[0-9][0-9\\.]*-?alpha-?[0-9]*)\\.jar$"},{"regex":"^(log4j-core-[0-9][0-9\\.]*-?beta-?[0-9]*)\\.jar$"},{"regex":"^(log4j-[0-9][0-9\\.]*)\\.jar$"},{"regex":"^(log4j-[0-9][0-9\\.]*-?rc-?[0-9]*)\\.jar$"},{"regex":"^(log4j-[0-9][0-9\\.]*-?alpha-?[0-9]*)\\.jar$"},{"regex":"^(log4j-[0-9][0-9\\.]*-?beta-?[0-9]*)\\.jar$"},{"regex":"^(apache-chainsaw-[0-9][0-9\\.]*)\\.jar$"},{"regex":"^(apache-chainsaw-[0-9][0-9\\.]*-?rc-?[0-9]*)\\.jar$"},{"regex":"^(apache-chainsaw-[0-9][0-9\\.]*-?alpha-?[0-9]*)\\.jar$"},{"regex":"^(apache-chainsaw-[0-9][0-9\\.]*-?beta-?[0-9]*)\\.jar$"},{"regex":"^(apache-log4j-extras-[0-9][0-9\\.]*)\\.jar$"},{"regex":"^(apache-log4j-extras-[0-9][0-9\\.]*-?rc-?[0-9]*)\\.jar$"},{"regex":"^(apache-log4j-extras-[0-9][0-9\\.]*-?alpha-?[0-9]*)\\.jar$"},{"regex":"^(apache-log4j-extras-[0-9][0-9\\.]*-?beta-?[0-9]*)\\.jar$"},{"regex":"^(elasticsearch-[0-9][0-9\\.]*)\\.jar$"},{"regex":"^(elasticsearch-[0-9][0-9\\.]*-?rc-?[0-9]*)\\.jar$"},{"regex":"^(elasticsearch-[0-9][0-9\\.]*-?alpha-?[0-9]*)\\.jar$"},{"regex":"^(elasticsearch-[0-9][0-9\\.]*-?beta-?[0-9]*)\\.jar$"},{"regex":"^(gt-main-[0-9][0-9\\.]*-?m-?[0-9]*)\\.jar$"},{"regex":"^(gt-main-[0-9][0-9\\.]*)-SNAPSHOT\\.jar$"},{"regex":"^(gt-main-[0-9][0-9\\.]*)\\.jar$"},{"regex":"^(gt-main-[0-9][0-9\\.]*-?rc-?[0-9]*)\\.jar$"},{"regex":"^(gt-main-[0-9][0-9\\.]*-?alpha-?[0-9]*)\\.jar$"},{"regex":"^(gt-main-[0-9][0-9\\.]*-?beta-?[0-9]*)\\.jar$"},{"regex":"^(jackson-databind-[0-9][0-9\\.]*[0-9]-[0-9][0-9]*)\\.jar$","format":"%1"},{"regex":"^(jackson-databind-[0-9][0-9\\.]*[0-9])-?(rc-?[0-9]*)\\.jar$","format":"%1-%2"},{"regex":"^(jackson-databind-[0-9][0-9\\.]*[0-9])-?(pr-?[0-9]*)\\.jar$","format":"%1-%2"},{"regex":"^(jackson-databind-[0-9][0-9\\.]*[0-9])\\.?(rc-?[0-9]*)\\.jar$","format":"%1-%2"},{"regex":"^(jackson-databind-[0-9][0-9\\.]*[0-9])\\.?(pr-?[0-9]*)\\.jar$","format":"%1-%2"},{"regex":"^(jackson-databind-[0-9][0-9\\.]*)\\.jar$"},{"regex":"^(jackson-databind-[0-9][0-9\\.]*-?rc-?[0-9]*)\\.jar$"},{"regex":"^(jackson-databind-[0-9][0-9\\.]*-?alpha-?[0-9]*)\\.jar$"},{"regex":"^(jackson-databind-[0-9][0-9\\.]*-?beta-?[0-9]*)\\.jar$"},{"regex":"^(commons-compress-[0-9][0-9\\.]*)\\.jar$"},{"regex":"^(commons-compress-[0-9][0-9\\.]*-?rc-?[0-9]*)\\.jar$"},{"regex":"^(commons-compress-[0-9][0-9\\.]*-?alpha-?[0-9]*)\\.jar$"},{"regex":"^(commons-compress-[0-9][0-9\\.]*-?beta-?[0-9]*)\\.jar$"},{"regex":"^(gson-[0-9][0-9\\.]*)\\.jar$"},{"regex":"^(gson-[0-9][0-9\\.]*-?rc-?[0-9]*)\\.jar$"},{"regex":"^(gson-[0-9][0-9\\.]*-?alpha-?[0-9]*)\\.jar$"},{"regex":"^(gson-[0-9][0-9\\.]*-?beta-?[0-9]*)\\.jar$"},{"regex":"^(guava-[0-9][0-9\\.]*)-jre\\.jar$"},{"regex":"^(guava-[0-9][0-9\\.]*)-android\\.jar$"},{"regex":"^(guava-[0-9][0-9\\.]*)-(rc-?[0-9]*)-android\\.jar$"},{"regex":"^(guava-r[0-9][0-9\\.]*)\\.jar$"},{"regex":"^(guava-[0-9][0-9\\.]*)\\.jar$"},{"regex":"^(guava-[0-9][0-9\\.]*-?rc-?[0-9]*)\\.jar$"},{"regex":"^(guava-[0-9][0-9\\.]*-?alpha-?[0-9]*)\\.jar$"},{"regex":"^(guava-[0-9][0-9\\.]*-?beta-?[0-9]*)\\.jar$"}]},"enabled_apps":["log4j-core","log4j","apache-chainsaw","apache-log4j-extras"],"supported_versions":["guava-28.1","elasticsearch-1.2.2","elasticsearch-6.2.4","gt-main-16-beta","elasticsearch-0.90.0.rc2","gt-main-2.7-m3","elasticsearch-7.17.8","elasticsearch-7.17.2","gt-main-28.1","elasticsearch-5.2.0","log4j-core-2.0-beta9","elasticsearch-7.10.0","elasticsearch-1.7.3","elasticsearch-0.7.1","gt-main-2.5-m2","jackson-databind-2.7.9.4","elasticsearch-5.6.3","elasticsearch-2.0.0-rc1","commons-compress-1.23.0","gt-main-14.3","log4j-core-2.11.2","elasticsearch-5.0.0-alpha4","gt-main-9.5","jackson-databind-2.8.4","gt-main-22.3","elasticsearch-5.2.2","elasticsearch-2.3.2","jackson-databind-2.9.3","log4j-core-2.1","log4j-1.2.4","gt-main-25-rc","log4j-1.2.16","elasticsearch-1.7.2","guava-31.0.1","gt-main-14-m0","log4j-core-2.0-rc1","jackson-databind-2.9.0-pr4","log4j-core-2.0-beta7","jackson-databind-2.0.1","elasticsearch-6.6.1","gt-main-10.7","elasticsearch-7.13.2","gson-2.2.2","jackson-databind-2.4.0-rc2","gt-main-20-rc","elasticsearch-7.0.0-alpha2","guava-32.0.1","elasticsearch-5.6.1","commons-compress-1.2","log4j-core-2.0-alpha2","gt-main-19.2","gt-main-2.5-m0","jackson-databind-2.2.4","gt-main-14-rc1","jackson-databind-2.8.8","jackson-databind-2.0.0-rc2","elasticsearch-7.2.0","gt-main-21.0","elasticsearch-0.15.0","gt-main-2.6.2","elasticsearch-5.6.13","elasticsearch-6.8.18","gt-main-29.2","guava-17.0-rc2","elasticsearch-7.6.0","elasticsearch-2.4.2","elasticsearch-7.11.2","gt-main-21.3","guava-19.0-rc2","guava-23.3","elasticsearch-7.17.7","elasticsearch-2.0.2","gson-1.1","jackson-databind-2.7.0-rc2","gt-main-10.2","log4j-core-2.13.1","elasticsearch-0.16.1","gt-main-13.1","elasticsearch-6.0.0-alpha2","gt-main-27.5","elasticsearch-6.4.0","jackson-databind-2.14.0-rc3","elasticsearch-7.17.11","gt-main-18.2","jackson-databind-2.15.0","log4j-1.2.1","gt-main-16-m0","jackson-databind-2.15.0-rc3","gt-main-2.5.1","elasticsearch-0.17.7","jackson-databind-2.12.7","log4j-core-2.10.0","elasticsearch-0.19.10","gt-main-20.5","elasticsearch-2.1.1","gt-main-2.5.0","elasticsearch-2.0.0","elasticsearch-0.20.1","commons-compress-1.11","elasticsearch-5.6.2","elasticsearch-6.2.1","gson-1.4","gt-main-29.1","gt-main-2.7.4","gt-main-26.1","jackson-databind-2.13.0","elasticsearch-7.3.2","gt-main-2.6-m1","guava-15.0","elasticsearch-0.90.7","elasticsearch-7.4.2","gt-main-2.6.3","gt-main-2.6-m3","guava-23.0","jackson-databind-2.11.0-rc1","elasticsearch-7.11.1","elasticsearch-2.4.5","gt-main-14-beta","jackson-databind-2.4.1.1","jackson-databind-2.0.6","jackson-databind-2.9.10.7","jackson-databind-2.2.0-rc1","log4j-core-2.3.2","elasticsearch-7.6.1","jackson-databind-2.1.2","elasticsearch-1.6.1","gt-main-11.1","jackson-databind-2.0.5","jackson-databind-2.3.0","gt-main-27.1","gt-main-2.7.0","elasticsearch-7.6.2","jackson-databind-2.13.3","jackson-databind-2.9.10.8","jackson-databind-2.6.0-rc4","commons-compress-1.1","elasticsearch-6.0.0-beta1","elasticsearch-7.14.0","elasticsearch-5.6.14","elasticsearch-0.10.0","gt-main-12.0","elasticsearch-0.90.6","gt-main-15.3","gt-main-8.4","guava-21.0-rc2","log4j-1.2.5","elasticsearch-1.5.0","elasticsearch-8.5.1","elasticsearch-2.4.4","gt-main-13.0","elasticsearch-0.20.3","elasticsearch-6.8.22","elasticsearch-7.17.3","elasticsearch-6.0.0-beta2","elasticsearch-7.3.0","guava-31.0","elasticsearch-2.4.0","log4j-core-2.14.1","jackson-databind-2.12.0","commons-compress-1.6","elasticsearch-1.3.0","jackson-databind-2.15.2","guava-18.0-rc1","guava-28.0","jackson-databind-2.6.7.2","gt-main-11.4","guava-27.0.1","elasticsearch-7.5.2","elasticsearch-1.7.6","gt-main-29-rc1","gt-main-10.8","jackson-databind-2.7.8","log4j-core-2.8","log4j-core-2.12.4","elasticsearch-5.4.3","gt-main-24.6","elasticsearch-6.1.4","gt-main-10.3","elasticsearch-0.17.10","gt-main-17.0","jackson-databind-2.14.0-rc2","log4j-core-2.5","elasticsearch-5.6.4","jackson-databind-2.5.3","jackson-databind-2.15.0-rc1","gt-main-10.6","elasticsearch-8.6.2","gt-main-2.7-m5","gt-main-24.7","guava-23.4","commons-compress-1.14","jackson-databind-2.12.0-rc2","elasticsearch-6.1.0","elasticsearch-1.2.4","jackson-databind-2.7.9.5","elasticsearch-8.1.0","gt-main-16.3","jackson-databind-2.15.1","gt-main-9.3","gt-main-19-rc1","guava-23.2","jackson-databind-2.8.2","jackson-databind-2.9.9","log4j-core-2.2","elasticsearch-7.8.1","gt-main-24-rc","gt-main-13.4","elasticsearch-6.2.2","jackson-databind-2.9.7","jackson-databind-2.6.7","jackson-databind-2.4.1.2","gson-2.10.1","guava-27.0","jackson-databind-2.8.3","elasticsearch-8.0.0","elasticsearch-7.9.0","log4j-core-2.4.1","elasticsearch-8.10.0","log4j-core-2.0","elasticsearch-5.0.2","gt-main-18-rc1","gt-main-9.4","elasticsearch-6.5.1","elasticsearch-8.6.0","elasticsearch-1.5.1","gt-main-12.5","gt-main-26.2","gt-main-11.2","guava-14.0","jackson-databind-2.8.0-rc2","jackson-databind-2.9.10.5","jackson-databind-2.3.0-rc1","jackson-databind-2.10.1","elasticsearch-7.0.0-alpha1","elasticsearch-1.4.0","elasticsearch-1.0.0","elasticsearch-6.8.5","elasticsearch-0.19.2","elasticsearch-5.6.10","elasticsearch-0.17.0","elasticsearch-5.0.0-alpha2","elasticsearch-7.4.1","jackson-databind-2.14.2","elasticsearch-6.8.0","gt-main-23.2","elasticsearch-0.90.11","commons-compress-1.16","elasticsearch-8.5.2","guava-12.0-rc2","guava-10.0-rc1","elasticsearch-8.0.0-alpha2","elasticsearch-1.1.1","elasticsearch-7.15.0","log4j-1.3alpha-7","elasticsearch-6.5.0","gson-2.2.1","elasticsearch-8.5.0","gt-main-29.0","elasticsearch-7.17.1","commons-compress-1.4.1","elasticsearch-6.8.19","gt-main-27.2","elasticsearch-6.8.16","gt-main-2.6.4","gt-main-25.3","elasticsearch-7.1.1","elasticsearch-1.5.2","gt-main-2.7.5","gt-main-12.1","elasticsearch-1.6.0","elasticsearch-0.16.3","jackson-databind-2.8.0","log4j-core-2.6.2","elasticsearch-0.90.13","elasticsearch-1.0.0.rc1","guava-32.1.1","gt-main-26.6","guava-13.0-rc2","elasticsearch-1.0.3","gt-main-11.0","jackson-databind-2.13.0-rc1","gt-main-2.7-m2","guava-29.0","jackson-databind-2.6.0-rc1","gt-main-17.3","gt-main-23.4","gt-main-15.4","gson-2.8.1","elasticsearch-8.7.1","jackson-databind-2.12.1","guava-r06","elasticsearch-8.3.2","elasticsearch-5.0.1","elasticsearch-8.0.0-rc2","gt-main-15-m0","elasticsearch-8.2.1","elasticsearch-7.9.3","elasticsearch-7.7.0","log4j-1.3alpha-6","elasticsearch-0.20.6","guava-23.0-rc1","elasticsearch-1.0.2","jackson-databind-2.13.5","elasticsearch-7.11.0","elasticsearch-6.8.11","elasticsearch-5.3.0","jackson-databind-2.13.2","elasticsearch-6.8.6","elasticsearch-0.18.3","elasticsearch-0.15.2","gt-main-2.5.3","log4j-core-2.3","guava-32.1.3","elasticsearch-7.17.0","elasticsearch-0.19.8","jackson-databind-2.2.1","elasticsearch-6.1.3","elasticsearch-6.4.3","elasticsearch-7.13.4","log4j-core-2.13.0","jackson-databind-2.11.4","elasticsearch-1.0.0.beta1","elasticsearch-6.8.10","jackson-databind-2.9.9.1","elasticsearch-0.17.2","commons-compress-1.7","gson-2.7","jackson-databind-2.8.10","elasticsearch-7.14.2","elasticsearch-0.18.2","elasticsearch-0.20.0","jackson-databind-2.13.2.1","elasticsearch-8.1.2","elasticsearch-1.4.3","elasticsearch-7.17.14","elasticsearch-6.8.12","elasticsearch-1.1.2","elasticsearch-0.17.9","gt-main-14.4","gt-main-17-beta","jackson-databind-2.5.4","jackson-databind-2.6.2","jackson-databind-2.6.7.3","jackson-databind-2.14.0-rc1","log4j-core-2.20.0","elasticsearch-0.14.2","elasticsearch-0.19.1","guava-22.0","commons-compress-1.19","elasticsearch-5.3.1","elasticsearch-7.10.2","log4j-1.2.13","jackson-databind-2.3.5","elasticsearch-1.7.1","gson-2.6.1","jackson-databind-2.15.3","gt-main-2.6.5","elasticsearch-0.19.12","elasticsearch-2.1.2","elasticsearch-8.10.2","elasticsearch-1.3.7","elasticsearch-1.7.4","elasticsearch-8.4.2","elasticsearch-6.0.0-alpha1","gt-main-8.0-rc2","elasticsearch-8.4.0","log4j-1.3alpha-5","log4j-core-2.11.0","elasticsearch-6.1.2","gt-main-8.0-m2","jackson-databind-2.8.0-rc1","gt-main-13-beta","gt-main-18.5","elasticsearch-7.17.10","log4j-core-2.0-beta4","elasticsearch-2.2.2","jackson-databind-2.8.11.1","jackson-databind-2.14.3","gt-main-9.0-beta1","gt-main-17.1","jackson-databind-2.6.6","jackson-databind-2.9.9.3","log4j-core-2.18.0","elasticsearch-1.0.1","jackson-databind-2.9.10.2","jackson-databind-2.15.0-rc2","jackson-databind-2.4.0-rc3","gt-main-10-beta","guava-14.0.1","elasticsearch-6.8.3","jackson-databind-2.9.10.3","gt-main-2.7.1","log4j-core-2.15.0","guava-18.0","elasticsearch-0.13.0","guava-11.0.1","elasticsearch-5.6.6","gt-main-28.4","gt-main-13.3","guava-r03","jackson-databind-2.9.10.4","guava-r07","guava-10.0.1","gson-2.8.4","gson-2.3.1","jackson-databind-2.0.0","gt-main-2.5-rc0","elasticsearch-0.18.6","elasticsearch-8.8.2","log4j-core-2.19.0","elasticsearch-0.19.5","gt-main-17.2","elasticsearch-0.20.4","jackson-databind-2.5.1","elasticsearch-6.0.0-rc2","log4j-core-2.13.2","gt-main-14.1","jackson-databind-2.7.5","elasticsearch-0.19.7","gt-main-2.7-beta1","log4j-core-2.0-alpha1","gt-main-2.7.3","gt-main-15.2","gt-main-12.4","jackson-databind-2.10.5.1","jackson-databind-2.7.9.2","gt-main-8.0","elasticsearch-5.6.5","gt-main-27.4","jackson-databind-2.2.3","elasticsearch-5.6.15","guava-16.0-rc1","elasticsearch-8.2.3","elasticsearch-0.18.5","commons-compress-1.20","guava-13.0","elasticsearch-5.1.1","guava-14.0-rc2","gson-2.8.3","gson-2.8.2","jackson-databind-2.7.7","jackson-databind-2.9.5","commons-compress-1.3","elasticsearch-0.17.4","gt-main-23.1","guava-10.0-rc2","elasticsearch-6.8.17","gt-main-16.5","commons-compress-1.10","gt-main-14.2","log4j-core-2.14.0","gt-main-24.4","guava-16.0","elasticsearch-7.16.1","jackson-databind-2.0.0-rc3","guava-21.0","elasticsearch-5.0.0-rc1","elasticsearch-7.3.1","gt-main-23-rc","jackson-databind-2.12.0-rc1","elasticsearch-2.3.0","gt-main-2.6.0","elasticsearch-8.2.2","log4j-core-2.12.2","elasticsearch-7.1.0","elasticsearch-0.19.4","gt-main-2.6.6","guava-14.0-rc1","elasticsearch-0.14.1","gt-main-9.1","gt-main-30.0","gt-main-8.6","jackson-databind-2.0.2","elasticsearch-8.5.3","commons-compress-1.0","jackson-databind-2.2.2","elasticsearch-6.0.0-rc1","elasticsearch-7.17.5","elasticsearch-5.5.0","gt-main-2.5.4","guava-23.6","gson-2.8.9","jackson-databind-2.9.9.2","gt-main-12.0.1","log4j-1.2.6","elasticsearch-0.18.1","elasticsearch-0.15.1","log4j-core-2.13.3","gt-main-2.7.2","gson-2.6.2","elasticsearch-1.2.3","jackson-databind-2.3.1","jackson-databind-2.9.6","elasticsearch-5.4.1","log4j-core-2.0-beta8","log4j-core-2.7","elasticsearch-0.16.4","jackson-databind-2.0.0-rc1","elasticsearch-6.8.15","log4j-core-2.12.3","elasticsearch-2.4.3","gson-2.1","gt-main-11-rc2","guava-24.0","gt-main-20.0","jackson-databind-2.12.6.1","gt-main-8.5","elasticsearch-6.8.23","jackson-databind-2.7.6","elasticsearch-7.13.1","jackson-databind-2.4.1.3","gt-main-20.3","elasticsearch-1.3.4","log4j-1.2.8","gt-main-14.0","gt-main-2.7-m1","gt-main-11.3","guava-28.2","elasticsearch-7.0.1","elasticsearch-0.90.8","elasticsearch-7.13.3","elasticsearch-1.3.3","log4j-core-2.0.2","jackson-databind-2.9.8","elasticsearch-6.8.20","elasticsearch-7.17.6","elasticsearch-0.19.6","elasticsearch-0.19.0","elasticsearch-6.8.13","guava-27.1","elasticsearch-5.0.0-alpha1","elasticsearch-8.4.1","elasticsearch-0.90.3","elasticsearch-8.0.1","gt-main-28.2","guava-10.0-rc3","guava-31.1","elasticsearch-2.1.0","gson-2.6","jackson-databind-2.11.2","jackson-databind-2.7.9","elasticsearch-5.4.2","log4j-1.3alpha-1","jackson-databind-2.9.0-pr2","jackson-databind-2.13.4","elasticsearch-0.6.0","jackson-databind-2.7.1-1","elasticsearch-8.4.3","gt-main-9.0-m0","commons-compress-1.21","commons-compress-1.24.0","jackson-databind-2.13.4.2","jackson-databind-2.3.3","elasticsearch-7.16.0","log4j-core-2.17.0","gt-main-25.5","gt-main-22.4","jackson-databind-2.9.2","elasticsearch-1.6.2","elasticsearch-1.4.0.beta1","gt-main-16.0","jackson-databind-2.6.0-rc3","elasticsearch-6.8.14","guava-25.1","elasticsearch-8.3.1","elasticsearch-0.19.0.rc2","gt-main-11-beta","gt-main-2.6.1","gt-main-2.7.0.2","elasticsearch-1.0.0.beta2","gt-main-16.1","jackson-databind-2.7.1","jackson-databind-2.4.3","elasticsearch-0.14.0","gt-main-10-rc1","gt-main-18.3","elasticsearch-0.18.0","elasticsearch-6.7.2","commons-compress-1.17","elasticsearch-0.14.4","log4j-1.3alpha-8","gt-main-24.5","gt-main-14.5","elasticsearch-1.3.9","jackson-databind-2.6.1","jackson-databind-2.8.6","jackson-databind-2.7.9.7","jackson-databind-2.9.0-pr1","elasticsearch-5.4.0","jackson-databind-2.5.5","elasticsearch-7.5.0","guava-30.1","jackson-databind-2.1.0","elasticsearch-6.7.1","elasticsearch-1.3.5","gt-main-14-m1","gt-main-20.1","jackson-databind-2.14.0","gson-2.4","jackson-databind-2.8.11.6","guava-10.0","elasticsearch-6.5.3","gt-main-21-rc","gt-main-12-beta","guava-24.1.1","jackson-databind-2.10.5","gt-main-2.5.6","gt-main-8.0-m1","elasticsearch-2.3.3","elasticsearch-5.6.9","gson-2.8.0","gt-main-8.2","gson-2.2.4","jackson-databind-2.12.7.1","jackson-databind-2.5.0","elasticsearch-0.17.5","jackson-databind-2.6.7.1","elasticsearch-0.90.4","elasticsearch-0.7.0","gt-main-26-rc","gt-main-15-beta","gt-main-8.1","jackson-databind-2.10.4","jackson-databind-2.9.0","guava-19.0-rc1","gt-main-18.1","elasticsearch-1.3.2","gson-2.9.1","log4j-core-2.17.2","guava-17.0","log4j-core-2.0-beta3","elasticsearch-5.6.11","gt-main-13-rc1","gt-main-19.0","log4j-core-2.9.0","gt-main-13.6","elasticsearch-8.2.0","elasticsearch-8.9.0","gt-main-21.4","guava-32.1.2","jackson-databind-2.4.0-rc1","gt-main-21.2","log4j-1.2.14","jackson-databind-2.13.2.2","jackson-databind-2.7.9.1","guava-13.0-rc1","log4j-1.2.17","jackson-databind-2.16.0-rc1","commons-compress-1.16.1","gt-main-8.0-rc1","gt-main-2.7.0.1","gson-2.3","elasticsearch-8.9.2","guava-11.0","log4j-core-2.9.1","elasticsearch-8.0.0-rc1","jackson-databind-2.4.6","jackson-databind-2.8.11","guava-19.0","gt-main-2.7-rc1","elasticsearch-7.10.1","log4j-1.2.9","jackson-databind-2.4.2","elasticsearch-5.5.2","gt-main-8.0-m4","guava-12.0.1","jackson-databind-2.1.5","commons-compress-1.13","elasticsearch-7.15.1","gt-main-2.7-rc2","elasticsearch-0.12.1","jackson-databind-2.9.1","guava-14.0-rc3","elasticsearch-0.90.10","elasticsearch-0.16.2","elasticsearch-0.20.0.rc1","jackson-databind-2.1.4","gt-main-24.1","elasticsearch-6.8.9","gt-main-24.3","jackson-databind-2.8.11.5","jackson-databind-2.4.4","gt-main-9.0-m1","elasticsearch-1.7.5","gt-main-12-rc1","gt-main-18.4","guava-23.6.1","commons-compress-1.5","elasticsearch-8.8.0","log4j-1.0.4","elasticsearch-1.2.1","gt-main-19.3","elasticsearch-5.3.2","jackson-databind-2.8.9","elasticsearch-6.6.0","log4j-core-2.0-beta1","jackson-databind-2.10.3","jackson-databind-2.3.2","elasticsearch-8.0.0-beta1","elasticsearch-1.3.6","elasticsearch-1.2.0","elasticsearch-5.5.3","gt-main-2.5-m3","log4j-core-2.8.2","guava-r08","elasticsearch-8.9.1","jackson-databind-2.12.3","gt-main-21.1","elasticsearch-6.7.0","gt-main-9.2","elasticsearch-7.9.2","jackson-databind-2.7.9.6","jackson-databind-2.7.3","log4j-core-2.6","log4j-1.2rc1","elasticsearch-7.0.0-beta1","jackson-databind-2.10.0-pr2","gt-main-2.6-m2","gt-main-16.4","gson-1.7.1","gson-1.7.2","guava-23.1","elasticsearch-7.15.2","elasticsearch-0.90.5","guava-22.0-rc1","elasticsearch-6.2.0","gt-main-28-rc","log4j-core-2.8.1","elasticsearch-0.16.0","gson-2.2.3","guava-25.0","elasticsearch-0.90.0.rc1","elasticsearch-0.90.12","elasticsearch-5.0.0","elasticsearch-5.6.0","log4j-1.3alpha-3","elasticsearch-8.10.3","elasticsearch-7.0.0","elasticsearch-2.2.1","gt-main-27.3","guava-11.0-rc1","guava-16.0.1","guava-32.1.0","jackson-databind-2.0.4","elasticsearch-1.0.0.rc2","gt-main-8.3","elasticsearch-0.19.0.rc3","gt-main-15.1","jackson-databind-2.8.5","jackson-databind-2.8.11.4","jackson-databind-2.6.0","jackson-databind-2.14.1","elasticsearch-5.6.16","gson-2.8.6","jackson-databind-2.7.0-rc1","jackson-databind-2.6.7.5","gt-main-23.3","elasticsearch-6.2.3","elasticsearch-2.2.0","gt-main-10.4","gt-main-10.0","gt-main-26.4","guava-12.0","elasticsearch-6.5.2","gt-main-11-rc1","gt-main-15-rc1","gson-2.8.7","gt-main-12.3","gson-2.5","gson-1.6","gt-main-15-beta2","elasticsearch-1.3.8","elasticsearch-0.14.3","elasticsearch-0.19.0.rc1","elasticsearch-6.8.21","elasticsearch-2.4.1","elasticsearch-8.0.0-alpha1","gt-main-25.0","guava-20.0","elasticsearch-7.17.4","elasticsearch-8.7.0","gt-main-24.2","elasticsearch-5.6.8","guava-18.0-rc2","jackson-databind-2.6.4","jackson-databind-2.6.7.4","gt-main-19.4","jackson-databind-2.7.4","jackson-databind-2.1.3","gt-main-25.4","elasticsearch-8.1.1","commons-compress-1.8.1","elasticsearch-8.1.3","guava-r09","jackson-databind-2.10.2","gt-main-10.1","jackson-databind-2.7.9.3","elasticsearch-5.2.1","gt-main-2.5-rc1","gt-main-16.2","gson-2.2","jackson-databind-2.4.5","gson-2.10","elasticsearch-1.3.1","jackson-databind-2.7.0","commons-compress-1.4","elasticsearch-5.0.0-alpha5","elasticsearch-6.3.0","jackson-databind-2.5.0-rc1","gt-main-18.0","jackson-databind-2.2.0","jackson-databind-2.4.6.1","gt-main-16-rc1","gt-main-26.3","guava-26.0","jackson-databind-2.10.0-pr1","log4j-1.2.15","elasticsearch-8.3.3","elasticsearch-7.4.0","jackson-databind-2.10.0-pr3","jackson-databind-2.9.10.6","elasticsearch-8.8.1","elasticsearch-7.0.0-rc1","elasticsearch-0.19.3","gt-main-12.2","gt-main-29.3","jackson-databind-2.6.5","guava-17.0-rc1","guava-11.0.2","gson-1.7","elasticsearch-0.90.1","elasticsearch-7.12.1","elasticsearch-0.20.2","guava-21.0-rc1","log4j-core-3.0.0-alpha1","elasticsearch-1.4.2","jackson-databind-2.8.11.2","elasticsearch-5.6.7","elasticsearch-7.9.1","log4j-core-2.3.1","jackson-databind-2.6.3","gt-main-10.5","elasticsearch-0.9.0","gt-main-2.5.5","gt-main-23.5","elasticsearch-0.17.3","guava-20.0-rc1","jackson-databind-2.4.1","elasticsearch-5.0.0-beta1","elasticsearch-0.17.1","jackson-databind-2.13.1","elasticsearch-8.10.1","gt-main-2.5.8","elasticsearch-7.17.12","jackson-databind-2.4.5.1","elasticsearch-6.6.2","jackson-databind-2.8.7","elasticsearch-2.0.1","commons-compress-1.12","commons-compress-1.8","elasticsearch-8.3.0","log4j-core-2.16.0","gt-main-25.6","log4j-1.2.11","commons-compress-1.15","gt-main-27.0","elasticsearch-6.8.8","elasticsearch-0.90.0.beta1","elasticsearch-0.90.2","elasticsearch-0.90.0","elasticsearch-0.16.5","gt-main-10-rc2","elasticsearch-5.5.1","elasticsearch-6.0.0","guava-15.0-rc1","elasticsearch-7.5.1","elasticsearch-8.10.4","gt-main-26.0","elasticsearch-0.19.11","elasticsearch-0.11.0","gt-main-22.0","elasticsearch-6.8.1","elasticsearch-0.17.6","commons-compress-1.22","jackson-databind-2.3.4","gt-main-25.2","gt-main-22-rc","elasticsearch-2.3.5","log4j-core-2.0-beta6","jackson-databind-2.12.4","elasticsearch-6.4.1","elasticsearch-7.16.3","gt-main-21.5","gt-main-28.3","gt-main-8.0-m3","gt-main-22.1","jackson-databind-2.8.1","log4j-core-2.12.1","gt-main-20.2","gt-main-28.5","gt-main-26.5","log4j-core-2.0-beta2","guava-32.0.0","guava-13.0.1","jackson-databind-2.7.0-rc3","jackson-databind-2.11.1","log4j-core-2.0.1","gt-main-27-rc","gson-2.0","commons-compress-1.18","gt-main-26.7","gt-main-13.5","jackson-databind-2.8.8.1","log4j-1.2beta4","guava-12.0-rc1","log4j-1.2.12","gt-main-2.6-rc1","gson-1.5","gt-main-19.1","gt-main-24.0","elasticsearch-7.7.1","elasticsearch-7.2.1","elasticsearch-7.8.0","log4j-core-2.11.1","jackson-databind-2.11.3","guava-19.0-rc3","gt-main-30-rc","elasticsearch-0.13.1","jackson-databind-2.1.1","elasticsearch-7.13.0","gt-main-2.5.7","gt-main-22.2","guava-30.1.1","jackson-databind-2.12.2","elasticsearch-6.0.1","log4j-core-2.12.0","elasticsearch-0.18.7","gt-main-13.2","log4j-core-2.17.1","log4j-core-2.0-rc2","jackson-databind-2.6.0-rc2","gson-2.8.8","guava-30.0","elasticsearch-5.3.3","gt-main-17.5","gson-2.8.5","elasticsearch-7.14.1","gt-main-8.7","elasticsearch-8.6.1","jackson-databind-2.12.5","elasticsearch-5.1.2","jackson-databind-2.5.2","elasticsearch-0.90.9","elasticsearch-6.8.2","elasticsearch-0.12.0","elasticsearch-7.17.9","elasticsearch-0.19.9","elasticsearch-1.4.1","elasticsearch-6.8.7","elasticsearch-0.20.5","gt-main-18-beta","elasticsearch-1.1.0","elasticsearch-6.8.4","jackson-databind-2.4.0","elasticsearch-1.4.5","gt-main-11.5","elasticsearch-2.4.6","gt-main-19-beta","gt-main-17-rc1","gt-main-2.7-m4","gt-main-2.7-m0","gt-main-21-m0","elasticsearch-6.3.2","elasticsearch-0.18.4","gt-main-9.0","elasticsearch-0.17.8","elasticsearch-6.4.2","jackson-databind-2.9.0-pr3","elasticsearch-2.0.0-beta1","elasticsearch-7.17.13","log4j-core-2.4","elasticsearch-1.7.0","elasticsearch-7.0.0-rc2","jackson-databind-2.9.10","elasticsearch-6.3.1","gt-main-22.5","gt-main-28.0","elasticsearch-2.3.4","log4j-1.2.7","elasticsearch-6.1.1","elasticsearch-5.0.0-alpha3","gt-main-8.0-m0","elasticsearch-2.3.1","gt-main-2.5.2","guava-23.5","log4j-1.2.3","elasticsearch-7.16.2","jackson-databind-2.13.0-rc2","jackson-databind-2.9.4","guava-r05","jackson-databind-2.7.2","elasticsearch-2.0.0-beta2","gt-main-17.4","gt-main-25.7","commons-compress-1.9","jackson-databind-2.10.0","elasticsearch-7.12.0","gson-2.9.0","gt-main-9.0-rc1","log4j-core-2.0-beta5","elasticsearch-1.4.4","jackson-databind-2.9.10.1","jackson-databind-2.11.0","gt-main-15.0","elasticsearch-6.5.4","log4j-core-2.6.1","gt-main-23.0","gt-main-25.1","guava-24.1","elasticsearch-0.8.0","jackson-databind-2.12.6","jackson-databind-2.13.4.1","elasticsearch-5.6.12","log4j-1.2.2","jackson-databind-2.8.11.3"]}
# javaclass.py 

def isClassFile(file):
    '''
    Returns True if the file handle is referencing a file that is a
    Java class file.  Returns False otherwise.
    '''
    magic = file.read(4)
    if len(magic) != 4:
        return False

    if magic[0] != 0xCA or magic[1] != 0xFE or magic[2] != 0xBA or magic[3] != 0xBE:
       return False
    return True

TAG_UTF8String = 1
TAG_Int32 = 3
TAG_Float = 4
TAG_Int64 = 5
TAG_Double = 6
TAG_ClassRef = 7
TAG_StringRef = 8
TAG_FieldRef = 9
TAG_MethodRef = 10
TAG_InterfaceMethodRef = 11
TAG_NameAndType = 12
TAG_MethodHandle = 15
TAG_MethodType = 16
TAG_Dynamic = 17
TAG_InvokeDynamic = 18
TAG_Module = 19
TAG_Package = 20

def decodeType(type, objName):
    ndx = 0
    res = []
    args = None
    retType = None
    isArray = False

    while ndx < len(type):
        c = type[ndx]
        name = None
        if c == 'I':
            name = 'int'
        elif c == 'Z':
            name = 'boolean'
        elif c == 'B':
            name = 'byte'
        elif c == 'C':
            name = 'char'
        elif c == 'S':
            name = 'short'
        elif c == 'J':
            name = 'long'
        elif c == 'F':
            name = 'float'
        elif c == 'D':
            name = 'double'
        elif c == 'V':
            name = ""
        elif c == 'L':
            ndx = ndx+1
            start=ndx
            while ndx < len(type) and type[ndx] != ';':
                ndx = ndx+1
            name = type[start:ndx]
        elif c == '[':
            isArray=True
            ndx = ndx + 1
            continue
        
        elif c == '(':
            ndx = ndx + 1
            start = ndx
            while ndx < len(type) and type[ndx] != ')':
                ndx = ndx+1
            args = '(' +  decodeType(type[start:ndx], None) + ')'
        else:
            print(type)
            raise TypeError("Invalid character '"+c+"'in type signature")

        if isArray:
            name = name + '[]'
        isArray = False

        if name is not None and args is None:
            res.append(name)
        else:
            retType = name
        ndx = ndx + 1


    if args:
        if retType == "":
            return objName + args
        else:
            return retType + " " + objName + args
    else:
        s = ",".join(res)
        if objName is None:
            return s
        elif s == "":
            return objName
        else:
            return s + " " + objName
            

class JavaClass:

    
    major_version = 0
    minor_version = 0
    
    __class = None
    __super = None
    __constants = []
    __methodInfo = []
    __fields = []
    __interfaces = []

    __NullSlot = False

    __file = None

    
    def __init__(self):
        self.__class = None
        self.__loaded = False
            
    def load(self, file):
        '''Loads a class file from a file handle'''
        if type(file) == type(b''):
            self.__file = BytesIO(file)
        else:
            self.__file = file
        data = self.__readbytes(4)
        if len(data) != 4:
            return None
        magic = self.__readlong(data)
        if magic != 0xCAFEBABE:
            return None

        self.minor_version = self.__readshort()
        self.major_version = self.__readshort()

        const_count = self.__readshort()

        #
        # To do this as a list comprehension, we need a bit of a hack
        # because of the TAG_Int64 TAG_Double stupidity.  These two tags
        # count as "two slots" in the constant pool, but are actually just
        # one slot.  So, we have to do two things.  One, not read anything
        # for the "second" slot, but because everything indexes into the
        # constant pool, we have to insert a dummy entry into the pool for
        # them.  self.__NullSlot is checked in readconstant() and doesn't
        # read anything, it just returns a dummy slot.  When we see a
        # TAG_Int64 or TAG_Double, we set self.__NullSlot to True
        #
        self.__NullSlot = False
        self.__constants = [self.__readconstant() for i in range(0,const_count-1)]

        self.__flags = self.__readshort()
        self.__class = self.__readshort()
        self.__super = self.__readshort()


        ifcount = self.__readshort()

        self.__interfaces = [self.__readshort() for i in range(0,ifcount)]

        fcount = self.__readshort()
        self.__fields = [self.__readfm() for i in range(0,fcount)]

        mcount = self.__readshort()
        self.__methodInfo = [self.__readfm() for i in range(0,mcount)]

        acount = self.__readshort()

        attr = [self.__readattr() for i in range(0,acount)]

        self.__file = None

        self.__loaded = True

        return self
        
    def loadFile(self,filename):
        '''Loads a class file specified by filename'''
        with open(filename, mode='rb') as file:
            self.load(file)
        return self

    #------------------------------------------------------------------------

    def is_loaded(self):
        return self.__loaded

    def get_class_name(self, raw=False):
        '''Returns the name of the class of the loaded class file'''
        if self.__class != 0:
            return self.__getClassName(self.__class, raw)
        return None

    def get_class_flags(self):
        return self.__flags
    
    def get_super_class_name(self, raw=False):
        '''Returns the name of the super class of the loaded class file'''
        if self.__super != 0:
            return self.__getClassName(self.__super, raw)
        return None

    def methods(self, raw=False):
        '''
        Returns a list of the defined method names in the class.

        Each element of the list contains a tuple consisting of the
        method name and the type signature of the method and the
        integer representation of the flags field.
        '''
        for x in self.__methodInfo:
            name = self.__getStrConst(x[1], raw)
            desc = self.__getStrConst(x[2], raw)
            yield (name, desc, x[0])


    def fields(self, raw=False):
        '''
        Returns a list of the defined fields in the class.

        Each element of the list contains a tuple consisting of the
        method name and the type signature of the method and the
        integer representation of the flags.
        '''
        for x in self.__fields:
            name = self.__getStrConst(x[1], raw)
            desc = self.__getStrConst(x[2], raw)
            yield (name, desc, x[0])
            

    def method_references(self, raw=False):
        '''
        Returns a list of methods referenced by the class.

        Each element of the list contains a 3 element tuple. The first
        element is the class where the called class is located.  The second
        element is the method name, and the third is the type signature of
        the method.
        '''

        for x in self.__constants:
            if x[0] == TAG_MethodRef:
                cname = self.__getClassName(x[1], raw)
                method,type = self.__getNameType(x[2],raw)
                yield (cname,method,type)

    def field_references(self, raw=False):
        for x in self.__constants:
            if x[0] == TAG_FieldRef:
                cname = self.__getClassName(x[1], raw)
                method,type = self.__getNameType(x[2],raw)
                yield (cname,method,type)
        

    def strings(self, raw=False):
        '''Returns a list of all the string constants in the class file'''

        for x in self.__constants:
            if x[0] != TAG_StringRef:
                continue
            s = self.__getStrConst(x[1], raw=True)
            if len(s) > 1:
                b = struct.unpack('B', s[0:1])[0]
                if b == 1:
                    continue
            yield self.__getStrConst(x[1], raw)

    def getPackageName(self, raw=False):
        '''Returns the package name'''
        cname = self.className(raw)
        if raw:
            n = cname.rfind(b'/')
        else:
            n = cname.rfind('/')
        if n == -1:
            return None
        return cname[0:n]

    def get_int_constants(self, raw=False):
        '''Returns a list of all the int32 constants in the class file'''

        if raw:
            for x in self.__constants:
                if x[0] == TAG_Int32 or x[0] == TAG_Int64:
                    yield x[2]
        else:
            for x in self.__constants:
                if x[0] == TAG_Int32 or x[0] == TAG_Int64:
                    yield x[1]

    def get_float_constants(self, raw=False):
        '''Returns a list of all the int32 constants in the class file'''

        if raw:
            for x in self.__constants:
                if x[0] == TAG_Float or x[0] == TAG_Double:
                    yield x[2]
        else:
            for x in self.__constants:
                if x[0] == TAG_Float or x[0] == TAG_Double:
                    yield x[1]

    def interfaces(self, raw=False):
        '''Returns a list of all implemented interfaces'''

        return map(lambda x: self.__getClassName(x,raw), self.__interfaces)

    
        
    #------------------------------------------------------------------------

    def __getClassName(self, index, raw=False):
        c = self.__constants[index-1]
        type = c[0]
        if type == TAG_ClassRef:
            return self.__getStrConst(c[1], raw)
        raise TypeError("Wrong type tag<"+str(type)+"> for class name record")

    def __getNameType(self, index, raw=False):
        c = self.__constants[index-1]
        type = c[0]
        if type == TAG_NameAndType:
            name = self.__getStrConst(c[1], raw)
            type = self.__getStrConst(c[2], raw)
            return (name,type)
        raise TypeError("Wrong type tag<"+str(type)+"> for name record")
        

    def __getMethodName(self, index, raw=False):
        c = self.__constants[index-1]
        type = c[0]
        if type == TAG_MethodRef:
            cname = self.__getClassName(c[1], raw)
            (name,type) = self.__getNameType(c[2], raw)
            return (cname, name, type)
        raise TypeError("Wrong type tag<"+str(type)+"> for method name record")

    def __getStrConst(self, index, raw=False):
        c = self.__constants[index-1]
        type = c[0]

        if type == TAG_UTF8String:
            if raw:
                return c[1]
            return c[1].decode("utf-8", errors='replace')
        raise TypeError("Wrong type tag<"+str(type)+"> for UTF-8 string constant")

    #------------------------------------------------------------------------

    def __readbytes(self, nbytes):
        res = None
        while nbytes > 0:
            d = self.__file.read(nbytes)
            n = len(d)
            if n <= 0:
                break
            if res is None:
                res = d
            else:
                res = res + d
            nbytes = nbytes - n
        if res is None:
            res = b''
        return res
    
    def __readshortd(self,data):
        return struct.unpack('>H', data[0:2])[0]

    def __readlong(self,data,offset=0):
        return struct.unpack('>I', data[offset:offset+4])[0]

    def __readshort(self):
        data = self.__readbytes(2)
        return self.__readshortd(data)

    def __read2short(self):
        data = self.__readbytes(4)
        a = self.__readshortd(data[0])
        b = self.__readshortd(data[2:])
        return (a,b)
    

    def __readfm(self):
        
        data = self.__readbytes(8)
        flags = self.__readshortd(data)
        index = self.__readshortd(data[2:])
        desc = self.__readshortd(data[4:])
        acount = self.__readshortd(data[6:])
        attr = [self.__readattr() for i in range(0,acount)]
        return (flags, index, desc, attr)

    def __readattr(self):

        data = self.__readbytes(6)
        index = self.__readshortd(data)
        alen = self.__readlong(data[2:])
        data = self.__readbytes(alen)
        return (index, data)

    def __readconstant(self):

        if self.__NullSlot is True:
            self.__NullSlot = False
            return (-1,None)

        data = self.__readbytes(3)
        tag = struct.unpack('B',data[0:1])[0]
        data = data[1:]
        if tag == TAG_UTF8String:
            dlen = self.__readshortd(data)
            data = self.__readbytes(dlen)
            return (tag, data)
                
        elif tag == TAG_NameAndType or tag == TAG_MethodRef or \
             tag == TAG_FieldRef or tag == TAG_InterfaceMethodRef:
            index1 = self.__readshortd(data)
            index2 = self.__readshort()
            return (tag,index1,index2)
                
        elif tag == TAG_ClassRef or tag == TAG_StringRef:
            index = self.__readshortd(data)
            return (tag,index)
                
        elif tag == TAG_Int32:
            data2 = self.__readbytes(2)
            a = self.__readshortd(data)
            b = self.__readshortd(data2)
            n = a << 16 | b
            return (tag, n, data+data2)
                
        elif tag == TAG_Float:
            data = data + self.__readbytes(2)
            v = struct.unpack('>f', data)[0]
            return (tag, v,data)
                
        elif tag == TAG_Int64:
            data = data + self.__readbytes(6)
            a = self.__readlong(data)
            b = self.__readlong(data, 4)
            v = a << 32 | b
            self.__NullSlot = True
            return(tag, v, data)

        elif tag == TAG_Double:
            data = data + self.__readbytes(6)
            v = struct.unpack('>d', data)[0]
            self.__NullSlot = True
            return(tag, v, data)
        
                
        elif tag == TAG_MethodHandle:
            data = data + self.__readbytes(1)
            return (tag, data)

        elif tag == TAG_MethodType:
            return (tag, data)
                
        elif tag == TAG_Dynamic:
            data = data + self.__readbytes(2)
            return (tag, data)
                
        elif tag == TAG_InvokeDynamic:
            data = data + self.__readbytes(2)
            return (tag, data)
                
        elif tag == tag == TAG_Package or tag == TAG_Module:
            index = self.__readshortd(data)
            return (tag, index)
                
        else:
            raise TypeError("Unrecognized tag<"+str(type)+"> while loading constant pool")

# jarversion.py 
#!/usr/bin/python


class JarNameVersion:

    def __init__(self, config):
        self.load_patterns(config)
        self.reported = set()
    
    def get_version(self, filename):
        filename = os.path.basename(filename).lower()
        for p,fmt in self.patterns:
            try:
                m = re.match(p, filename)
            except:
                if p not in self.reported:
                    sys.stderr.write("Malformed regular expression " + p + "\n")
                    self.reported.add(p)
                continue
            
            if m is None:
                continue
            if fmt is None:
                fmt="%1"

            pos = 0
            segments = [x for x in m.groups()]
            replacements = []
            for ndx,value in enumerate(segments,1):
                var = '%' + str(ndx)
                value = segments[ndx-1]
                pos = 0
                while True:
                    ndx = fmt.find(var, pos)
                    if ndx == -1:
                        break
                    replacements.append((value,ndx,ndx+len(var)))
                    pos = ndx + len(var)

            v = []
            pos = 0
            for value, start, end in replacements:
                v.append(fmt[pos:start])
                v.append(value)
                pos = end
                
            version = "".join(v)
                
            return version
        return None

    def load_patterns(self, config):
        with open(config, 'r', encoding='utf8') as f:
            monitored = json.load(f)

        standard = [
            "-[0-9][0-9\.]*)\.jar$",
            "-[0-9][0-9\.]*-?rc-?[0-9]*)\.jar$",
            "-[0-9][0-9\.]*-?alpha-?[0-9]*)\.jar$",
            "-[0-9][0-9\.]*-?beta-?[0-9]*)\.jar$"
        ]

        patterns = set()
        for name in monitored.keys():
            rec = monitored[name]
            if not rec['enabled']:
                continue
            for std in standard:
                patterns.add(("(" + name + std,None))

            if 'match' in rec:
                for r in rec['match']:
                    rex = r['regex']
                    if 'format' in r:
                        patterns.add((rex, r['format']))
                    else:
                        patterns.add((rex, None))
        self.patterns = patterns


# jardata.py 
#!/usr/bin/python



class JarDataExtract:
    def __init__(self):
        self.synthetics = set()
        self.object_overrides = set()
        self.class_elements = {}
        self.class_digests = {}
        self.class_counter = 0

    @classmethod
    def hexnyb(cls, n):
        return "0123456789abcdef"[n]

    @classmethod
    def hexbyte(cls, b):
        a = (b >> 4) & 15
        b = b & 15
        return cls.hexnyb(a) + cls.hexnyb(b)

    @classmethod
    def hex(cls, data):
        res=[]
        for b in struct.unpack(str(len(data))+"B", data):
            res.append(cls.hexbyte(b))
        return "".join(res)
        
    

    def get_class_fingerprints(self, jc):

        elements = set()

        flags = jc.get_class_flags()
        #
        # Ignore synthetic classes
        #

        if (flags & 0x1000) != 0:
            return None
        
        cname = jc.get_class_name()

        if cname == 'module-info':
            return None
        
        if cname is not None:
            elements.add('cn:' + cname)

        sclass = jc.get_super_class_name()
        if sclass is not None:
            elements.add("pc:" + sclass)
        s = str(flags)
        elements.add("cf:" + s)

        for interface in jc.interfaces():
            elements.add("if:" + interface)

        for name,descr,flags in jc.fields():            
            if flags & 0x1000 != 0:
                key = "F:" + cname + "." + name
                self.synthetics.add(key)
                continue
            flags = str(flags)
            elements.add("fd:" + name + ";" + descr + ";" + flags)
            prefix="fd:" + name + ":"

        for name,descr,flags in jc.methods():
            
            key = "M:" + cname + ";" + name + ";" +descr

            if name in ['clone', 'equals', 'finalize', 'getClass', 'hashCode', 'notify', 'notifyAll', 'toString']:
                self.object_overrides.add(key)
                
            if flags & 0x1040 != 0:
                self.synthetics.add(key)
                continue
            
            flags = str(flags)
            elements.add("md:" + name + ";" + descr + ";" + flags)
            prefix="md:" + name + ";" + descr + ":"

        for s in jc.strings():
            elements.add("sc:" + s)

        for n in jc.get_int_constants():
            elements.add("iv:" + str(n))

        for n in jc.get_float_constants():
            s = "{0:0.4f}".format(n)
            elements.add("fv:" + s)

        for cname,mname,descr in jc.method_references():

            key = "M:" + cname + "." + name + ";" +descr

            if mname == '$values':
                continue


            #
            # Ignore return type
            #
            ndx = descr.find(")")
            if ndx != -1:
                descr = descr[0:ndx+1]
            elements.add("mr:" + cname + "." + name + ";" + descr)

        for cname,fname,descr in jc.field_references():
            if fname.find('$SwitchMap$') != -1:
                continue

            elements.add("fr:" + cname + "." + name + ";" + descr)

        classname = jc.get_class_name()
        self.class_elements[classname] = sorted(list(elements))
        self.class_counter += 1

    def get_class_count(self):
        return self.class_counter

    def get(self):

        class_fingerprints = []
        digest_set = []
        fingerprint_set = []

        #
        # Need to handle method references to local instances that
        # override java.lang.Object methods.
        #
        #if mname in ['clone', 'equals', 'finalize', 'getClass', 'hashCode', 'notify', 'notifyAll', 'toString']:
        #   if key not in self.object_overrides:
        #        continue

        allnames = set()
        for cname in self.class_digests:
            allnames.add(cname)
        for cname in self.class_elements:
            allnames.add(cname)

        for cname in sorted(allnames):
            if cname in self.class_elements:
                h = sha256()
                for e in sorted(filter(lambda x : x not in self.synthetics,
                                self.class_elements[cname])):
                    #
                    # Temporary work around for Python2 / Python3
                    # differences in handling non-ASCII
                    #
                    if False and not all(c in string.printable for c in e):
                        continue
                    
                    if e[0:3] != 'mr:':
                        h.update(e.encode(encoding='utf8'))
                        continue
                    mr = e.split(';')[0]
                    mname = mr.split('.')[-1]

                    if mname in ['clone', 'equals', 'finalize', 'getClass', 'hashCode', 'notify', 'notifyAll', 'toString']:                    
                        if e not in self.object_overrides:
                            continue
                        
                    h.update(e.encode(encoding='utf8'))
                    
                fingerprint = self.hex(h.digest())
                fingerprint_set.append(fingerprint)
            else:
                fingerprint = None

            rec = {
                'class': cname,
                'fingerprint': fingerprint
            }

            if cname in self.class_digests:
                digest = self.class_digests[cname]
                digest_set.append(digest)
                rec['digest'] = digest
            

            class_fingerprints.append(rec)

        if len(digest_set) != 0:
            h = sha256()
            for d in sorted(digest_set):
                h.update(d.encode('utf8'))
            jar_digest = self.hex(h.digest())
        else:
            jar_digest = None

        
        h = sha256()
        for d in sorted(fingerprint_set):
            h.update(d.encode('utf8'))
        fingerprint = self.hex(h.digest())

        #
        # jar-class-digest: Digest of the sorted digests of the
        #                   raw bytes of all class files.
        # jar-fingerprint:  Digest of the sorted fingerprints of
        #                   all of the class files.
        # classes:          Individual class information:
        #
        #    fingerprint:   Fingerprint of the class file
        #    digest:        Digest of the raw bytes of the class file.
        #    class:         Name of the class
        #
        
        rec = {
            'jar-fingerprint': fingerprint,
            'jar-class-digest': jar_digest,
            'classes': class_fingerprints,
        }

        return rec


    def add_class_file(self, filehandle):
        h = sha256()
        classbytes = BytesIO()
        while True:
            d = filehandle.read(-1)
            if d is None:
                continue
            if len(d) == 0:
                break
            h.update(d)
            classbytes.write(d)
        digest = self.hex(h.digest())
        classbytes.seek(0,0)
        jc = JavaClass()
        jc.load(classbytes)
        classname = jc.get_class_name()
        self.class_digests[classname] = digest
        self.get_class_fingerprints(jc)

    def get_jar_fingerprints(self, zipHandle):

        for f in zipHandle.infolist():
            zfn = f.filename
            if not zfn.endswith('.class'):
                continue
            if zfn.startswith("META-INF/"):
                continue
            with zipHandle.open(zfn) as zf:
                self.add_class_file(zf)
        return self.get()
    

# textreport.py 


class TextReport:

    @classmethod
    def name(cls):
        return "text"

    def __init__(self, args):

        use_color=args['use_color']
        
        if not use_color:
            self.red="<Alert>"
            self.orange="<Attention>"
            self.yellow="<Notice>"
            self.reset=""
        else:
            self.red="[91m"
            self.orange="[33m"
            self.yellow="[93m"
            self.reset="[39;49m"

        if sys.version_info.major == 3:
            self.vbar =  chr(0x2502)
            self.hbar =  chr(0x2500)
            self.angle = chr(0x2514)
            self.joint = chr(0x251c)
            self.cross = chr(0x253c)
        else:
            self.vbar = '|'
            self.hbar = '-'
            self.angle = '+'
            self.joint = '+'
            self.cross = '+'
        
        self.output=[]

    def get_color(self, score):
        if score >= 7.5:
            return self.red

        if score >= 5:
            return self.orange

        if score >= 2.5:
            return self.yellow

        return None

    def indent(self, indents, blank=False):
        if len(indents) == 0:
            return
        
        for indent in indents[0:-1]:
            if indent:
                self.output.append("  " + self.vbar + "  ")
            else:
                self.output.append("     ")

        bar = self.hbar
        if blank:
            self.output.append("  " + self.vbar + "\n")
            return
            
        if indents[-1]:
            self.output.append("  " + self.joint + self.hbar + self.hbar)
        else:
            self.output.append("  " + self.angle + self.hbar + self.hbar) 
                

    def convert(self, record, indents=[]):

        type = record['type']
        name = record['name']

        self.indent(indents)

        self.output.append(type+":"+name+"\n")
        if 'versions' in record:
            nversions = len(record['versions'])
            for ndx in range(0,nversions):
                v = record['versions'][ndx]
                vindent = indents + [ndx < (nversions-1)]
                self.indent(vindent)
                color=None

                if 'cve' in v:
                    for c in sorted(v['cve'],key=lambda x: x['score'], reverse=True):
                        color = self.get_color(c['score'])
                        if color is not None:
                            break

                if color is None:
                    color=""

                vnames = " or ".join(v['version'].split(":"))
                self.output.append("version: "+color+vnames+self.reset+"\n")
                if 'cve' in v:
                    cve_list = sorted(v['cve'],key=lambda x: x['score'], reverse=True)
                    ncve = len(cve_list)
                    for cndx in range(0,ncve):
                        c = cve_list[cndx]
                        cve_indent = vindent + [cndx < (ncve-1)]
                        id = c['id']
                        level = c['severity']
                        score = c['score']
                        color = self.get_color(score)
                        self.indent(cve_indent)
                        self.output.append(color+id+self.reset+" ["+level+"/"+str(score)+"]"+"\n")
                    if ndx < (nversions-1):
                        self.indent(vindent, blank=True)

        if 'children' in record:
            nchild = len(record['children'])-1
            for ndx in range(0,nchild+1):
                c = record['children'][ndx]
                self.convert(c, indents=indents + [ndx < nchild])


    def get(self):
        return "".join(self.output)
# config.py 

class Configuration:

    @classmethod
    def get_analytic_data(cls, name):

        return ConfigurationData.analytic_data.get(name, None)
# inputs.py 


class Input:
    __name = None
    __file = None
    __children = None
    __versions = None
    __parent = None
    __hidden = False
    __type = None
    __comment = None
    __fullname = None
    __appname = None
    __traits = None
    __displayname = None
    
    def __init__(self,name,type,fileHandle):
        self.__name = name
        self.__type = type
        self.__file = fileHandle
        self.__children = None
        self.__versions = None
        self.__parent = None
        self.__hidden = False
        self.__comment = None
        self.__fullname = None
        self.__appname = None
        self.__traits = None
        self.__displayname = name

    def isFile(self):
        return False

    def addAppName(self, name, evidence):
        if self.__appname is None:
            self.__appname = set()
        self.__appname.add((name,evidence))

    def setName(self, name):
        self.__name = name

    def setDisplay(self,name):
        self.__displayname = name

    def getDisplay(self):
        return self.__displayname

    def setType(self, t):
        self.__type = t

    def close(self):
        self.__file.close()

    def getFullName(self):
        if self.__fullname is None:
            return self.__name
        return self.__fullname

    def setFullName(self,fn):
        self.__fullname = fn

    def setStream(self, handle):
        self.__file = handle

    def addTraits(self, traits):
        if self.__traits is None:
            self.__traits = []
        self.__traits.append(traits)

    def getName(self):
        return self.__name

    def setHidden(self,status):
        self.__hidden = status

    def getHandle(self):
        return self.__file

    def setComment(self, comment):
        self.__comment = comment

    def hasVersions(self):
        if self.__versions is not None:
            return True
        if self.__children is not None:
            for c in self.__children:
                if c.hasVersions():
                    return True
        return False

    def to_dict(self):
        if self.__hidden:
            if self.__children is None:
                return {}
            return self.__children[0].to_dict()

        res = {}
        res['type'] = self.__type
        res['name'] = self.__displayname

        if self.__name != self.__displayname:
            res['alternate_name'] = self.__name

        if self.__comment is not None:
            res['comment'] = self.__comment

        if self.__appname is not None and not self.hasVersions():
            aset=[]
            for (app,ev) in self.__appname:
                x = {}
                x['appname'] = app
                if ev is not None:
                    x['evidence'] = ev
                aset.append(x)
            res['application'] = aset
            
        if self.__children is not None and len(self.__children) != 0:
            cset = []
            for child in self.__children:
                cset.append(child.to_dict())
            res['children'] = cset

        if self.__versions is not None:
            vset = []
            for v in self.__versions:
                vset.append(self.__versions[v].to_dict())
            res['versions'] = vset

        if self.__traits is not None:
            res['traits'] = self.__traits

        return res

    def toJSON(self):
        return json.dumps(self.to_dict(),separators=(',',':'))

    def addChild(self,child):
        if self.__children is None:
            self.__children = []
        self.__children.append(child)
        child.__parent = self

    def clean(self):
        if self.__getvcount() != 0:
            return
        if self.__parent is None:
            return
        if self.__traits is not None:
            return
        self.__parent.__rmchild(self)

    def __rmchild(self,child):
        if self.__children is None:
            return

        self.__children.remove(child)
        child.__parent = None


    def __getvcount(self):
        total = 0
        if self.__versions is not None:
            total = total + len(list(self.__versions))
        if self.__appname is not None:
            total = total + len(list(self.__appname))

        if self.__children != None:
            for child in self.__children:
                total = total + child.__getvcount()
        return total

    def addVersion(self, version):
        if self.__versions is None:
            self.__versions = {}
        v = version.getVersionID()
        if v not in self.__versions:
            self.__versions[v] = version
        else:
            self.__versions[v].add(version)

class FileInput(Input):

    def __init__(self,filename,name,type):
        f = open(filename,"rb")
        Input.__init__(self, filename, type, f)
        self.setFullName(filename)
        self.setDisplay(name)

    def isFile(self):
        return True
# tarfilereader.py 


class TarFileReader(IOBase):

    __handle = None
    __fileSize = None
    __filename = None
    __origSize = None
    __padding = None
    __filePos = 0

    def __init__(self, handle, bytes, name=None):
        self.__filename = name
        self.__handle = handle
        self.__filePos = 0
        self.__fileSize = bytes
        self.__origSize = bytes
        self.__filePos = 0
        p = bytes % 512
        if p != 0:
            p = 512 - p
        self.__padding = p

    def close(self):
        if self.__handle is not None:
            remaining = self.__fileSize - self.__filePos
            remaining = remaining + self.__padding

            while remaining > 0:
                b = self.__handle.read(remaining)
                if b is None:
                    break
                n = len(b)
                if n <= 0:
                    break
                remaining = remaining - n
            self.__handle = None

    def read(self, size = -1):
        remaining = self.__fileSize - self.__filePos 
        if size == -1:
            size = remaining
        elif size > remaining:
            size = remaining
        res = b''
        while size > 0:
            b = self.__handle.read(size)
            if b is None:
                break
            n = len(b)
            res = res + b
            size = size - n
            self.__filePos = self.__filePos + n
        return res

    def writable(self):
        return False

    def readable(self):
        return True

    def seekable(self):
        return False

    def __enter__(self):
        return 0

    def __exit__(self):
        return 0

    def __iter__(self):
        raise UnsupportedOperation("Not supported")

    def __next__(self):
        raise UnsupportedOperation("Not supported")

    def readlines(self):
        raise UnsupportedOperation("Not supported")

    def readline(self):
        raise UnsupportedOperation("Not supported")

    def writelines(self):
        raise UnsupportedOperation("Not supported")

    def isatty(self):
        return False

    def flush(self):
        raise UnsupportedOperation("Not supported")
# digest.py 

class DigestInput(IOBase):

    __handle = None
    __digest = None

    def __init__(self, handle, digester):
        self.__handle = handle
        self.__digest = digester

    def close(self):
        if self.__handle is not None:
            while True:
                b = self.__handle.read(512)
                if b is None:
                    break
                if len(b) == 0:
                    break
                self.__digest.add(b)
        self.__handle = None
        self.__digest.finish()

    def read(self, size = -1):
        data = self.__handle.read(size)
        if data is not None:
            self.__digest.add(data)
        return data

    def isFile(self):
        return False

    def writable(self):
        return False

    def readable(self):
        return True

    def seekable(self):
        return False

    def __enter__(self):
        return 0

    def __exit__(self):
        return 0

    def __iter__(self):
        raise UnsupportedOperation("Not supported")

    def __next__(self):
        raise UnsupportedOperation("Not supported")

    def readlines(self):
        raise UnsupportedOperation("Not supported")

    def readline(self):
        raise UnsupportedOperation("Not supported")

    def writelines(self):
        raise UnsupportedOperation("Not supported")

    def isatty(self):
        return False

    def flush(self):
        raise UnsupportedOperation("Not supported")
# javajar.py 

def isJarFile(file):
    lfn = file.lower()

    if lfn.endswith(".jar"):
        return True
    if lfn.endswith(".jpi"):
        return True
    if lfn.endswith(".war"):
        return True
    if lfn.endswith(".ear"):
        return True
    if lfn.endswith(".hpi"):
        return True
    if lfn.endswith(".sar"):
        return True
    if lfn.endswith(".kar"):
        return True
    if lfn.endswith(".par"):
        return True
    return False

def getFileType(file):
    lfn = file.lower()
    
    if lfn.endswith(".jar"):
        return "jar"
    if lfn.endswith(".zip"):
        return "zip"
    if lfn.endswith(".tar"):
        return "tar"
    if lfn.endswith(".tar.gz"):
        return "tar"
    if lfn.endswith(".tgz"):
        return "tar"
    if lfn.endswith(".class"):
        return "class"
    if lfn.endswith(".pom"):
        return "pom"
    if lfn.endswith(".jpi"):
        return "jpi"
    if lfn.endswith(".war"):
        return "war"
    if lfn.endswith(".ear"):
        return "ear"
    if lfn.endswith(".hpi"):
        return "hpi"
    if lfn.endswith(".sar"):
        return "sar"
    if lfn.endswith(".kar"):
        return "kar"
    if lfn.endswith(".par"):
        return "par"
    
# procinfo.py 


class ProcInfo:
    __sysType = None

    #
    # Add OS detection here
    #
    def __init__(self):
        platform = os.sys.platform
        if platform[0:5] == 'linux':
            self.__sysType = LinuxProcInfo()
        elif platform == 'darwin' or platform[0:3] == "aix":
            self.__sysType = MacProcInfo()
        elif platform[0:5] == 'win32':
            self.__sysType = Win32ProcInfo()
        else:
            raise NotImplementedError("No ProcInfo support for platform "+platform)

    #
    # Get list of process IDS
    #
    def getPids(self):
        '''
        Returns a list of process ids (as strings).
        '''
        return self.__sysType.getPids()
    
    #
    # Get the process environment variables as list of (key,value)
    #
    def getEnviron(self,pid):
        '''
        Returns a list of the environment variables of the given process
        identifier.  Each element of the list is a tuple consisting of the
        variable name and the variable value.
        '''
        return self.__sysType.getEnviron(pid)

    #
    # Get the command line arguments as a list
    #
    def getCommandLine(self,pid):
        '''
        Returns the command line of the specified process as a list of
        strings for each component of the command line.
        '''
        return self.__sysType.getCommandLine(pid)

    #
    # Return a list of filenames that the process has open
    #
    def getOpenFiles(self,pid):
        '''
        Returns a list containing the names of the files that the
        specified process currently has open.
        '''
        return self.__sysType.getOpenFiles(pid)

    #
    # Get the name of the executable
    #
    def getExecutable(self,pid):
        '''
        Returns the name of the file containing the executable associated
        with the given process.
        '''
        return self.__sysType.getExecutable(pid)

    #
    # Get current working directory
    #
    def getCWD(self,pid):
        '''
        Returns the current working directory of the given process.
        '''
        return self.__sysType.getCWD(pid)

    #
    # Check if process is in the same file system name space
    #
    def isSameNS(self,pid):
        '''
        Returns True if the process is in the same file system name
        space as the calling program.
        '''
        return self.__sysType.isSameNS(pid)
    

class LinuxProcInfo():

    def getPids(self):
        pids=[]
        for file in os.listdir("/proc"):
            if file.isdigit():
                pids.append(file)
        return pids

    def getEnviron(self,pid):
        env=[]
        with open("/proc/"+str(pid)+"/environ", "rb") as f:
            data = f.read()
            start = None
            end = None
            for ndx in range(0,len(data)):
                if data[ndx] == 0:
                    s = data[start:end].decode("UTF-8")
                    name,val = s.split('=',1)
                    if name.find(' ') == -1:
                        env.append((name,val))
                    start=None
                    end=None
                else:
                    if start is None:
                        start = ndx
                        end = ndx+1
                    else:
                        end = end + 1
            if start is not None:
                s = data[start:end].decode("UTF-8")
                if name.find(' ') == -1:
                    name,val = s.split('=',1)
                env.append((name,val))

        return env

    def getCommandLine(self,pid):
        args=[]
        with open("/proc/"+str(pid)+"/cmdline", "rb") as f:
            data = f.read()
            start = None
            end = None
            for ndx in range(0,len(data)):
                bd = data[ndx:ndx+1]
                b = struct.unpack('B',bd)[0]
                if b == 0:
                    args.append(data[start:end].decode("UTF-8"))
                    start=None
                    end=None
                else:
                    if start is None:
                        start = ndx
                        end = ndx+1
                    else:
                        end = end + 1
            if start is not None:
                args.append(data[start:end].decode("UTF-8"))
        if len(args) == 0:
            return None
        return args
    
    def getOpenFiles(self,pid):
        dir="/proc/"+str(pid)+"/fd"
        files=[]
        for file in os.listdir(dir):
            fsn = dir+"/"+file
            try:
                name = os.readlink(fsn)
                files.append((fsn,name))
            except:
                continue
            
        return files

    def getExecutable(self,pid):
        try:
            return os.readlink("/proc/"+str(pid)+"/exe")
        except:
            return None

    def getCWD(self,pid):
        try:
            return os.readlink("/proc/"+str(pid)+"/cwd")
        except:
            return None

    def isSameNS(self,pid):
        try :
            mine = os.readlink("/proc/self/ns/mnt")
            pids = os.readlink("/proc/"+str(pid)+"/ns/mnt")
            if mine == pids:
                return True
            return False
        except:
            return False



class MacProcInfo():

    __cache={}

    def getPids(self):
        pids=[]
        self.__cache = {}
        p = subprocess.Popen(["ps","axww"], stdout=subprocess.PIPE)
        psout=[]
        if sys.version_info.major == 3:
            with TextIOWrapper(p.stdout, encoding="utf-8") as pin:
                for line in pin:
                    psout.append(line)
        else:
            for line in p.stdout:
                psout.append(line)

        for line in psout:
            line = line.rstrip()
            line = line.strip()
            while line.find('  ') != -1:
                line = line.replace('  ',' ')
            try:
                pid, a,b,c, cmdline = line.split(' ',4)
                if pid != "PID":
                    pids.append(pid)
                    args = cmdline.split(' ')
                    self.__cache[pid] = {}
                    self.__cache[pid]['cmdline'] = args
                    self.__cache[pid]['exe'] = args[0]
            except:
                pass

        p.wait()
        
        return pids

    def getEnviron(self,pid):
        env=[]
        return env

    def getCommandLine(self,pid):
        if pid in self.__cache:
            return self.__cache[pid]['cmdline']
        return None
    
    def getOpenFiles(self,pid):
        files = []
        if os.path.exists("/usr/bin/procfiles"):
            p = subprocess.Popen(["/usr/bin/procfiles","-nc",pid], stdout=subprocess.PIPE)
            lines=[]
            if sys.version_info.major == 3:
                with TextIOWrapper(p.stdout, encoding="utf-8") as pin:
                    for line in pin:
                        lines.append(line)
            else:
                for line in p.stdout:
                    lines.append(line)
            for line in lines:
                line = line.rstrip()
                line = line.strip()
                if line.startswith("---"):
                    continue
                while line.find('  ') != -1:
                    line = line.replace('  ',' ')
                f = line.split(' ')
                if f[0] == 'FD':
                    continue
                if f[1] != '-':
                    continue
                file = f[-1]
                files.append((file,file))
            p.wait()
        elif os.path.exists("/usr/bin/lsof"):
            p=subprocess.Popen(["/usr/bin/lsof","-p",pid], stdout=subprocess.PIPE)
            lines=[]
            if sys.version_info.major == 3:
                with TextIOWrapper(p.stdout, encoding="utf-8") as pin:
                    for line in pin:
                        lines.append(line)
            else:
                for line in p.stdout:
                    lines.append(line)
            for line in lines[1:]:
                line = line.rstrip()
                line = line.strip()
                while line.find('  ') != -1:
                    line = line.replace('  ',' ')
                file = line.split(' ',8)[-1]
                files.append((file,file))
            p.wait()
        
        return files

    def getExecutable(self,pid):
        if pid in self.__cache:
            return self.__cache[pid]['exe']
        return None

    def getCWD(self,pid):
        return None

    def isSameNS(self,pid):
        return True


class Win32ProcInfo():

    __cache={}

    def getPids(self):
        pids=[]
        self.__cache = {}
        self.__haveOpenFiles = False
        p = subprocess.Popen(["C:\\Windows\\System32\\wbem\\WMIC.exe","process","get","processId,executablepath,commandline"], stdout=subprocess.PIPE)
        psout=[]
        if sys.version_info.major == 3:
            with TextIOWrapper(p.stdout) as pin:
                for line in pin:
                    psout.append(line)
        else:
            for line in p.stdout:
                psout.append(line)


        header = psout[0]

        columns={}

        start=0
        foundSpace=False
        for n in range(0,len(header)):
            if header[n] == ' ':
                if not foundSpace:
                    name = header[start:n]
                foundSpace = True
            elif foundSpace:
                columns[name]=(start,n-1)
                start = n
                foundSpace=False

        for line in psout[1:]:
            fields={}
            for fname in columns:
                (s,e) = columns[fname]
                val = line[s:e]
                val = val.rstrip()
                fields[fname] = val
            pid = fields['ProcessId']
            if pid == '':
                continue
            pids.append(pid)
            self.__cache[pid] = {}
            self.__cache[pid]['cmdline'] = fields['CommandLine'].split(' ')
            self.__cache[pid]['exe'] = fields['ExecutablePath']

        p.wait()
        
        return pids

    def getEnviron(self,pid):
        env=[]
        return env

    def getCommandLine(self,pid):
        if pid in self.__cache:
            return self.__cache[pid]['cmdline']
        return None
    
    def getOpenFiles(self,pid):
        if not self.__haveOpenFiles:
            self.__haveOpenFiles = True

            self.__openfiles = {}

            p = subprocess.Popen(["C:\\Windows\\System32\\openfiles.exe","/query","/fo","CSV","/v"], stdout=subprocess.PIPE)
            psout=[]
            if sys.version_info.major == 3:
                with TextIOWrapper(p.stdout) as pin:
                    for line in pin:
                        if line[0] == '"':
                            psout.append(line.rstrip())
            else:
                for line in p.stdout:
                    if line[0] == '"':
                        psout.append(line.rstrip())

            if len(psout) == 0:
                return None

            csvr = csv.reader(psout)
            for row in csvr:
                pid = row[2]
                filename = row[4]
                if pid not in self.__openfiles:
                    self.__openfiles[pid] = set()
                self.__openfiles[pid].add(filename)

        if pid not in self.__openfiles:
            pid = str(pid)
            if pid not in self.__openfiles:
                return []
        return [(fn,fn) for fn in self.__openfiles[pid]]

    def getExecutable(self,pid):
        if pid in self.__cache:
            return self.__cache[pid]['exe']
        return None

    def getCWD(self,pid):
        return None

    def isSameNS(self,pid):
        return True

    

# syspackages.py 

class SysPackages:

    # RPM: rpm -qal --files-bypkg -> pkg filename
    # DPKG: dpkg -S 'glob'  ->  pkg: filename
    #
    @classmethod
    def get_file_names(cls, patterns):
        isDPKG=False
        isRPM=False
        isLPP=False
        cmd=None
        pkgtype=None
        if os.path.exists("/usr/bin/lslpp"):
            isLPP=True
            pkgtype='lpp'
            cmd=["/usr/bin/lslpp", "-f", "-c"]
        elif os.path.exists("/usr/bin/dpkg"):
            isDPKG=True
            pkgtype = 'deb'
            cmd=["/usr/bin/dpkg","-S","*"]
        elif os.path.exists("/usr/bin/rpm"):
            isRPM=True
            pkgtype='rpm'
            cmd=["/usr/bin/rpm", "-qal", "--files-bypkg"]
        else:
            return None, 0

        if sys.version_info.major == 3:
            p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.DEVNULL)
        else:
            p = subprocess.Popen(cmd, stdout=subprocess.PIPE)
            
        lines=[]
        if sys.version_info.major == 3:
            with TextIOWrapper(p.stdout, encoding="utf-8") as pin:
                for line in pin:
                    lines.append(line.strip())
        else:
            for line in p.stdout:
                lines.append(line.strip())

        result = {}
        result[pkgtype] = {}

        counter = 0
        for rec in lines:
            if isDPKG:
                rec = rec.replace(':', ' ')
            while rec.find('  ') != -1:
                rec = rec.replace('  ',' ')
            if isLPP and rec[0] == '#':
                continue
            if isLPP:
                (pkg,file) = rec.split(':',2)[1:]
            else:
                (pkg,file) = rec.split(' ',1)
            for p in patterns:
                if re.search(p, file) is not None:
                    if pkg not in result[pkgtype]:
                        result[pkgtype][pkg] = []
                    result[pkgtype][pkg].append(file)
                    counter += 1
                    break

        return result, counter
# scanfs.py 

def isOfInterest(fn):

    if isJarFile(fn):
        return True

    if fn == 'METADATA':
        return True

    if fn == 'PKG-INFO':
        return True

    lfn = fn.lower()

    if lfn.endswith(".class"):
        return True

    
    if State.scanZipFiles and lfn.endswith(".zip"):
        return True
    
    if State.scanTarFiles:
        if lfn.endswith(".tar"):
            return True
        if lfn.endswith(".tgz"):
            return True
        if lfn.endswith(".tar.gz"):
            return True
        
    if State.scanPomFiles and lfn.endswith(".pom"):
        return True
    
    return False

def scanfs(fs, skipset, inputHandle):

    n = len(fs)
    if fs[-1] == '/':
        n = n - 1

    fsh = FileSystem()

    for path, file in fsh.walkFileSystem([fs], skipset, match=lambda fn : isOfInterest(fn)):
        tpath = path[n+1:]
        pfile = path + "/" + file
        if len(tpath) != 0:
            dname=pfile
        else:
            dname=file
        try:
            sin = FileInput(pfile, dname, getFileType(file))
            inputHandle.addChild(sin)
            try:
                dispatchFile(sin)
            except Exception as e:
                errorOut("FS["+pfile+"]:"+str(e))
                #traceback.print_exc()

            sin.clean()
            sin.close()
        except Exception as e:
            errorOut(pfile+":"+str(e))

# filesystem.py 


class FileSystem:

    def getFileSystems(self, filter=None, invert=False):
        res = []
        platform = os.sys.platform
        if platform[0:5] == 'win32':
            p = subprocess.Popen(["C:\\Windows\\System32\\wbem\\WMIC.exe","logicaldisk","get","deviceid,drivetype"], stdout=subprocess.PIPE)
            psout=[]
            if sys.version_info.major == 3:
                with TextIOWrapper(p.stdout) as pin:
                    for line in pin:
                        psout.append(line)
            else:
                for line in p.stdout:
                    psout.append(line)

            header = psout[0]

            columns={}

            start=0
            foundSpace=False
            for n in range(0,len(header)):
                if header[n] == ' ':
                    if not foundSpace:
                        name = header[start:n]
                    foundSpace = True
                elif foundSpace:
                    columns[name]=(start,n-1)
                    start = n
                    foundSpace=False

            for line in psout[1:]:
                fields={}
                for fname in columns:
                    (s,e) = columns[fname]
                    val = line[s:e]
                    val = val.rstrip()
                    fields[fname] = val
                t = fields['DriveType']
                mtpoint = fields['DeviceID']+'\\'
                type='unknown'
                if t == '1':
                    type = 'no-root-dir'
                elif t == '2':
                    type = 'removable-disk'
                elif t == '3':
                    type = 'fixed-local-disk'
                elif t == '4':
                    type = 'network-drive'
                elif t == '5':
                    type = 'cdrom'
                elif t == '6':
                    type = 'ramfs'
                keep = False
                
                if filter is None or type in filter:
                    keep = True
                if invert:
                    keep = not keep
                if keep:
                    res.append(mtpoint)
            p.wait()
            return res
            
        if os.path.exists("/etc/mtab") and os.path.getsize("/etc/mtab") != 0:
            with open('/etc/mtab') as file:
                for line in file:
                    line = line.rstrip()
                    dev, mtpoint, type = line.split(' ')[0:3]
                    keep = False
                    if filter is None or type in filter:
                        keep = True
                    if invert:
                        keep = not keep
                    if keep:
                        mtpoint = mtpoint.replace('\\040', ' ')
                        res.append(mtpoint)
        else:
            mount=None
            for b in ["/bin/mount", "/sbin/mount", "/usr/bin/mount", "/usr/sbin/mount"]:
                if os.path.exists(b):
                    mount=b
                    break
            if mount is None:
                return None
            p = subprocess.Popen([mount], stdout=subprocess.PIPE)
            lines = []
            if sys.version_info.major == 3:
                with TextIOWrapper(p.stdout, encoding="utf-8") as pin:
                    for line in pin:
                        lines.append(line)
            else:
                for line in p.stdout:
                    lines.append(line)
            isAIX=False
            if os.sys.platform[0:3] == "aix":
                isAIX = True
            for line in lines:
                line = line.rstrip()
                line = line.strip()
                while line.find('  ') != -1:
                    line = line.replace('  ',' ')
                f = line.split(" ")
                if isAIX:
                    if f[0][0] == '-':
                        continue
                    if f[0] == "node":
                        continue
                    mtpoint = f[1]
                    type = f[2]
                else:
                    mtpoint = f[2]
                    type = f[4]

                keep = False
                if filter is None or type in filter:
                    keep = True
                if invert:
                    keep = not keep
                if keep:
                    res.append(mtpoint)

                
            p.wait()
        return res


    def walkFileSystem(self, start, stop=None, match=None):
        for dir in start:
            for path, subdirs, files in os.walk(os.path.normpath(dir)):
                if stop is not None:
                    for n in subdirs:
                        if path[-1] == '/':
                            pn = path + n
                        else:
                            pn = path + '/' + n
                        if pn in stop:
                            subdirs.remove(n)

                for f in files:
                    keep = True
                    if match is not None:
                        if not match(f):
                            keep = False
                        if keep:
                            yield (path, f)

# jar_identifier.py 

class JarIdentifier:
    #
    # Called at startup for any initialization the class may need to do
    #
    # Returns: None
    #
    @classmethod
    def initialize(cls):
        return

    #
    # Called to verify that this analytic is supported.  This typically
    # means checking to see if the configuration contains the tables
    # needed for this analytic.  It can also be used to check for other
    # requirements.
    #
    # Returns: True or False
    #
    @classmethod
    def supported(cls):
        errorOut(cls.get_name() + " did not define supported() method. Disabling.")
        return False

    #
    # Define the priority of this analytic.  Priority is used only to
    # determine which analytic is the default if no analytic is
    # specified.  The analytic with the highest priority is the
    # default.  The current overall highest priority is 100
    # (class-fingerprint).
    #
    # Returns: Integer priority
    #
    @classmethod
    def priority(cls):
        return -1

    #
    # The name of the analytic
    #
    # Returns: String name
    #
    @classmethod
    def get_name(cls):
        raise NotImplementedError(str(cls)+": get_name() must be implemented")

    #
    # A brief description of the analytic
    #
    # Returns: String description
    #
    @classmethod
    def get_description(cls):
        return "No description provided"

    #
    # Does this analytic need to consume the raw data of an input stream?
    # An input stream is typically a Jar file.
    #
    # Returns: True or False
    #
    @classmethod
    def scans_input_stream(cls):
        return False

    #
    # Does this analytic need to consume the raw data of Java class file?
    #
    # Returns: True or False
    #
    @classmethod
    def scans_class_file(cls):
        return False
    
    #
    # Called if scans_input_stream() or scans_class_file is True; it
    # normally would save the input stream as the input source for the
    # analytic.  It would create a new stream that it forwards the
    # data read into.  This new input stream is returned to the
    # caller.
    #
    # This will create a "pipeline" of streams
    #
    # Returns: A new stream handle
    #
    def add_input_stream(self,streamIn):
        return streamIn


    #
    # Does the analytic need the Java class file to be decoded?
    #
    # Returns: True or False
    #
    @classmethod
    def uses_class_file(cls):
        return False

    #
    # If uses_class_file is True, then a JavaClass object will be
    # created from the class file and passed to add_class_file().
    # 
    # Returns: None
    #
    def add_class_file(self,cf):
        return

    #
    # Called after a jar file has been completely processed to allow
    # the analytic to do final version determination.
    #
    # Returns: none
    #
    def identify(self,inputHandle):
        raise NotImplementedError("identify() must be implemented")

# jar_name.py 

class JarName(JarIdentifier):

    myname="jar-name"
    
    def __init__(self):
        self.regex_map = Configuration.get_analytic_data('jar-name')['identifiers']
        self.reported_bad = set()

    @classmethod
    def priority(cls):
        return 5
        
    @classmethod
    def supported(cls):
        if Configuration.get_analytic_data(cls.myname):
            return True
        return False        

    @classmethod
    def initialize(cls):
        cls.config = Configuration.get_analytic_data('jar-name')
        if cls.config is None:
            return False
        cls.regex_map = cls.config['identifiers']
        return True
    
    def identify(self,inputHandle):
        names = set()
        names.add(inputHandle.getName())
        names.add(inputHandle.getDisplay())

        res = False

        for name in names:
            n = name.rfind("/")
            if n != -1:
                name = name[n+1:]
            name = name.lower()

            for rule in self.regex_map:
                regex = rule['regex']
                fmt = rule.get('format', '%1')

                try:
                    m = re.match(regex, name)
                except Exception:
                    if regex not in self.reported_bad:
                        errorOut("Malformed regular expression: {regex}\n")
                        self.reported_bad.add(regex)
                    continue
                if m is None:
                    continue

                pos = 0
                segments = [x for x in m.groups()]
                replacements = []
                for ndx,value in enumerate(segments,1):
                    var = '%' + str(ndx)
                    value = segments[ndx-1]
                    pos = 0
                    while True:
                        ndx = fmt.find(var, pos)
                        if ndx == -1:
                            break
                        replacements.append((value,ndx,ndx+len(var)))
                        pos = ndx + len(var)

                v = []
                pos = 0
                for value, start, end in replacements:
                    v.append(fmt[pos:start])
                    v.append(value)
                    pos = end
                v = "".join(v)

                nv = Version(v, self.myname)
                if State.addEvidence:
                    nv.addEvidence(self.myname, "Filename matched /"+regex+"/")
                inputHandle.addVersion(nv)
                res = True
                break
        
        return res

    @classmethod
    def get_name(cls):
        return cls.myname

    @classmethod
    def get_description(cls):
        return "Uses patterns to extract the version from the name of the jar file. This analytic is not robust against file renaming, and also can not detect when class files have been included directly in a larger project."
# jar_digest.py 

class JarDigest(JarIdentifier):

    myname="jar-digest"
    
    def __init__(self):
        self.hashes = []
        self.stream = None
        return

    @classmethod
    def priority(cls):
        return 70
    
    @classmethod
    def supported(cls):
        if Configuration.get_analytic_data(cls.myname):
            return True
        return False

    @classmethod
    def initialize(cls):
        cls.config = Configuration.get_analytic_data('jar-digest')
        if cls.config is None:
            return False
        cls.versionmap = cls.config['identifiers']
        return True

    @classmethod
    def scans_class_file(cls):
        return True
    
    @classmethod
    def get_name(cls):
        return cls.myname

    @classmethod
    def get_description(cls):
        return "Uses cryptographic digests (SHA256) of the contents of the jar file to identify the version of the jar file. This analytic is not robust against compilation using different Java compilers or different options. It also can not detect when class files have been included directly in a larger project."

    #------------------------------------------------------------------------


    #
    # Called by DigestInput for any data that is read from the stream
    #
    def add(self, data):
        self.digester.update(data)
        self.nbytes += len(data)
    #
    # Called by DigestInput when the stream is closed()
    #
    def finish(self):
        d = hex(self.digester.digest())
        self.hashes.append(d)
        self.digester = None
        self.stream = None

    #
    # Called by class file handler for each class file
    #
    def add_input_stream(self, stream):
        if self.stream is not None:
            self.stream.close()
            
        self.nbytes = 0
        self.digester = sha256()
        self.stream = DigestInput(stream, self)
        return self.stream
        

    #
    # Called after jar file has been completely scanned
    #
    def identify(self,inputHandle):

        if self.stream is not None:
            self.stream.close()

        if len(self.hashes) == 0:
            return False
        
        h = sha256()
        for hv in sorted(self.hashes):
            h.update(hv.encode('utf8'))

        size = self.config['size']
        jar_digest = hex(h.digest())[0:size]

        if jar_digest in self.versionmap:
            pmap = self.config['prefix-map']
            for p,v in self.versionmap[jar_digest]:
                if p != -1:
                    v = pmap[p] + v
                nv = Version(v, self.myname)
                if State.addEvidence:
                    nv.addEvidence(self.myname, "Digest matched "+jar_digest)
                inputHandle.addVersion(nv)
            return True
            
        return False

# jar_fingerprint.py 

class JarFingerprint(JarIdentifier):

    myname="jar-fingerprint"
    
    def __init__(self):
        self.jde = JarDataExtract()
        return

    @classmethod
    def priority(cls):
        return 80    

    @classmethod
    def supported(cls):
        if Configuration.get_analytic_data(cls.myname):
            return True
        return False    

    @classmethod
    def initialize(cls):
        cls.config = Configuration.get_analytic_data('jar-fingerprint')
        if cls.config is None:
            return False
        cls.versionmap = cls.config['identifiers']
        return True


    @classmethod
    def uses_class_file(cls):
        return True

    @classmethod
    def get_name(cls):
        return cls.myname

    @classmethod
    def get_description(cls):
        return "Uses string constants, method names, field names, referenced methods and referenced fields to create a fingerprint of each class. These fingerprints are then used to identify the version of the jar file containing them. This analytic is robust against compilation with different Java compilers and different compiler options. It also is able to identify versions where the class files have been included directly in a larger project."

    #------------------------------------------------------------------------

    def add_class_file(self,cf):
        self.jde.get_class_fingerprints(cf)

    def identify(self,inputHandle):

        if self.jde.get_class_count() == 0:
            return False
        
        info = self.jde.get()
        size = self.config['size']
        fingerprint = info['jar-fingerprint'][0:size]

        if fingerprint not in self.versionmap:
            return None

        info = self.versionmap[fingerprint]
        prefixes = self.config['prefix-map']

        versions = []
        for pid,v in info:
            if pid != -1:
                v = prefixes[pid] + v
            versions.append(v)

        for v in versions:
            nv = Version(v, self.myname)
            if State.addEvidence:
                nv.addEvidence(self.myname, "Fingerprint matched "+fingerprint)
            inputHandle.addVersion(nv)

    
# class_fingerprint.py 

class ClassFingerprint(JarIdentifier):

    myname="class-fingerprint"
    
    def __init__(self):
        self.jde = JarDataExtract()
        return

    @classmethod
    def priority(cls):
        return 100
    
    @classmethod
    def supported(cls):
        if Configuration.get_analytic_data(cls.myname):
            return True
        return False

    @classmethod
    def initialize(cls):
        cls.config = Configuration.get_analytic_data('class-fingerprint')
        if cls.config is None:
            return False
        cls.versionmap = cls.config['identifiers']
        return True

    @classmethod
    def uses_class_file(cls):
        return True

    @classmethod
    def get_name(cls):
        return cls.myname

    @classmethod
    def get_description(cls):
        return "Uses string constants, method names, field names, referenced methods and referenced fields to create a fingerprint of each class. These fingerprints are then used to identify the version of the jar file containing them. This analytic is robust against compilation with different Java compilers and different compiler options. It also is able to identify versions where the class files have been included directly in a larger project."

    #------------------------------------------------------------------------

    def add_class_file(self,cf):
        self.jde.get_class_fingerprints(cf)

    def identify(self,inputHandle):

        if self.jde.get_class_count() == 0:
            return False

        info = self.jde.get()
        size = self.config['size']

        versions={}
        unknown=0

        name = inputHandle.getDisplay()

        pkgmap = self.config['package-map']
        prefix_map = self.config['prefix-map']
        class_map = self.config['class-map']

        for crec in info['classes']:
            fingerprint = crec['fingerprint'][0:size]

            if fingerprint not in self.versionmap:
                unknown += 1
                continue

            for pid, cid, vid, version in self.versionmap[fingerprint]:
                cname = class_map[cid]
                if pid == -1:
                    continue
                else:
                    package = pkgmap[pid]

                if package == '':
                    continue

                if vid != -1:
                    prefix = prefix_map[vid]
                else:
                    prefix = ""

                if prefix not in versions:
                    versions[prefix] = {}
                vi = versions[prefix]
                
                if version not in vi:
                    vi[version] = {}
                vi = vi[version]
                
                if package not in vi:
                    vi[package] = set()
                vp = vi[package]
                
                vp.add(cname)

        matches={}
        for p in versions:
            max = 0
            candidates = []

            version_info = self.config['version_info']
            package_data = {}
            for v in versions[p]:
                version_name = p + v

                x = version_info[version_name]['packages']

                count = 0
                packages = set()
                
                for pkg in versions[p][v]:
                    if pkg not in version_info[version_name]['packages']:
                        continue
                    packages.add(pkg)
                    count += len(versions[p][v][pkg])

                if count > max:
                    max = count
                    candidates = [version_name]
                    package_data[version_name] = list(packages)
                elif count == max:
                    package_data[version_name] = list(packages)
                    candidates.append(version_name)
                    
            if max == 0:
                continue
            
            for v in sorted(candidates):
                nc = version_info[v]['class_count']
                nv = Version(v, self.myname)
                if max < nc/4:
                    nv.add_note(str(max) + " of " + str(nc) + " classes from " + v + " embedded in this jar file.")
                elif max < (nc*3/4):
                    nv.add_note(str(max) + " of " + str(nc) + " classes from " + v + " found in this jar file.")
                    
                if State.addEvidence:
                    pkgs = package_data[v]
                    if max < nc:
                        nv.addEvidence(self.myname, str(max) + " fingerprints out of "+str(nc)+" found.")
                        nv.addEvidence(self.myname, "Packages: " + ",".join(pkgs))
                    else:
                        nv.addEvidence(self.myname, "Fingerprints of all class files found ("+str(max)+"/"+str(nc)+").")
                inputHandle.addVersion(nv)

    
# class_digest.py 

class ClassDigest(JarIdentifier):

    myname="class-digest"

    def __init__(self):
        self.stream = None
        self.digester = None
        self.hashes = []
        return

    @classmethod
    def priority(cls):
        return 90

    @classmethod
    def supported(cls):
        if Configuration.get_analytic_data(cls.myname):
            return True
        return False

    @classmethod
    def initialize(cls):
        cls.config = Configuration.get_analytic_data('class-digest')
        if cls.config is None:
            return False
        cls.versionmap = cls.config['identifiers']
        return True

    @classmethod
    def scans_class_file(cls):
        return True

    def add_input_stream(self,streamIn):
        if self.stream is not None:
            self.stream.close()
            
        self.digester = sha256()
        self.stream = DigestInput(streamIn, self)
        
        return self.stream

    def add(self, data):
        self.digester.update(data)

    def finish(self):
        digest = hex(self.digester.digest())
        self.hashes.append(digest)
        self.digester = None
        self.stream = None

    def identify(self,inputHandle):

        if self.stream is not None:
            self.stream.close()

        if len(self.hashes) == 0:
            return False
        
        candidates = {}
        size = self.config['size']
        
        self.hashes = [h[0:size] for h in self.hashes]


        pkgmap = self.config['package-map']
        prefix_map = self.config['prefix-map']
        class_map = self.config['class-map']

        for h in self.hashes:
            if h not in self.versionmap:
                continue
            for pid,cid,vid,v in self.versionmap[h]:
                cname = class_map[cid]
                
                if pid == -1:
                    if cname == 'module-info':
                        continue
                    package = ''
                else:
                    package = pkgmap[pid]

                if vid != -1:
                    prefix = prefix_map[vid]
                else:
                    prefix = ""

                version = prefix + v

                if version not in candidates:
                    candidates[version] = 1
                else:
                    candidates[version] += 1

        max = 0
        for v in candidates:
            if candidates[v] > max:
                max = candidates[v]

        if max == 0:
            return False

        version_info = self.config['version_info']
        for version in candidates:
            
            if candidates[version] != max:
                continue
            nv = Version(version, self.myname)
            nc = version_info[version]['class_count']
            if max < nc:
                nv.add_note(str(max) + " of " + str(nc) + " classes from " + version + " embedded in this jar file.")
            if State.addEvidence:
                nv.addEvidence(self.myname, "Digests of class files found.")
            inputHandle.addVersion(nv)

        return True

    @classmethod
    def get_name(cls):
        return cls.myname

    @classmethod
    def get_description(cls):
        return "Uses cryptographic digests (SHA256) of class files in the jar file to identify the version. It is not robust against recompilation, however it can identify versions where the class files have been included directly in a larger project, barring recompilation."
# check_class.py 

def checkClass(inputHandle):
    #
    # Add code here to handle loose class files
    #
    if len(State.currentJar) == 0:
        State.looseClasses.append(inputHandle.getFullName())
        return False

    stream = inputHandle.getHandle()

    #
    # For now, skip class files that are in META-INF/version
    #
    filename = inputHandle.getFullName()
    if filename.find("META-INF/version") != -1:
        return False

    needClassLoaded = False
    if len(State.currentJar) != 0:
        for a in State.currentJar[-1]:
            if a.scans_class_file():
                stream = a.add_input_stream(stream)
                if stream is None:
                    errorOut(a.get_name() + ".add_input_stream() returned None")
                needClassLoaded = True
            elif a.uses_class_file():
                needClassLoaded = True


    if needClassLoaded:
        jcf = JavaClass().load(stream)
        if jcf is None:
            return False

    analytics = State.currentJar[-1]

    for a in analytics:
        if a.uses_class_file():
            a.add_class_file(jcf)
            
    return True

# classcollect.py 


class ClassGrouping:
    #
    # Tries to determine version from a group of unpackaged class files
    # which seem to belonog to the same package
    #
    def classPkgVersion(self,cpkg, inputHandle):
        '''Tries to determine version from a group of unpackaged class files
           which seem to belonog to the same package.  groupClassFiles can
           be used for the grouping.  The input is a list of class filenames.
        '''

        analytics=[]
        for a in State.analyticSet:
            name = a.get_name()
            if name in State.enabledAnalytics:
                analytics.append(a())

        State.currentJar.append(analytics)

        jc = JavaClass()
        for d in cpkg:
            for f in cpkg[d]:
                jcf = jc.loadFile(f)
                if jcf is None:
                    continue

                analytics = State.currentJar[-1]

                for a in analytics:
                    a.addClassFile(jcf)
                
        for a in analytics:
            a.identify(inputHandle)

        State.currentJar.pop()

    #
    # Group unpackaged class files by package name
    #
    def groupClassFiles(self,classes):
        '''Group unpackaged class files by package name

        Attempts to group the provide list of class names based on
        the packages they belong to.  They are also separated based
        on file system directory as well.  The returned value
        is a double dictionary.  The top level dictionary is keyed
        by directory name.  The second level is the package name.  This
        points to a list of classes.  So result[dir][pkgname] -> [classes].
        '''


        pkginfo={}
        jc = JavaClass()
        for cn in classes:
            try:
                cf = jc.loadFile(cn)
                pkg = cf.getPackageName()
                if pkg is None:
                    pkg="<none>/"
                if pkg not in pkginfo:
                    pkginfo[pkg] = []
                pkginfo[pkg].append(cn)
            except:
                pass

        pkgs={}
        for pkg in pkginfo:
            p = pkg.split('/')
            pname=None
            depth=0
            for x in range(0,len(p)):
                s = '/'.join(p[0:x])
                if s not in pkginfo:
                    pname = s
                    depth = 0
                else:
                    break
            if pname not in pkgs:
                pkgs[pname] = []
            for x in pkginfo[pkg]:
                pkgs[pname].append(x)

        pkgdir={}
        for pkg in pkgs:
            for f in pkgs[pkg]:
                n = f.find(pkg)
                if n == -1:
                    dirname="/"
                else:
                    dirname=f[0:n]
                if dirname not in pkgdir:
                    pkgdir[dirname] = {}
                if pkg not in pkgdir[dirname]:
                    pkgdir[dirname][pkg] = []
                pkgdir[dirname][pkg].append(f)

        return pkgdir
# dispatch.py 


def dispatchFile(inputHandle, fromUser=False):

    name = inputHandle.getDisplay()

    if isJarFile(name):
        return processJar(inputHandle)

    lname = name.lower()

    if lname.endswith(".zip"):
        if State.scanZipFiles or fromUser:
            return checkZip(inputHandle)
        return False

    if lname.endswith(".tar"):
        if State.scanTarFiles or fromUser:
            try:
                state = checkTar(inputHandle)
                return state
            except TarSparseUnsupported as e:
                errorOut("DIS1:"+str(e))
            
        return False

    if lname.endswith(".tgz") or lname.endswith(".tar.gz"):
        if State.scanTarFiles or fromUser:
            if inputHandle.isFile():
                zin = gzip.open(name, mode='rb')
            else:
                zin = gzip.GzipFile(fileobj=inputHandle.getHandle())
                
            h = Input(name, "gz", zin)
            inputHandle.addChild(h)
            h.setHidden(True)
            status = False
            try:
                status = checkTar(h)
                return status
            except TarSparseUnsupported as e:
                errorOut("DIS2:"+str(e))
            h.clean()
        return False

    if lname.endswith(".class"):
        return checkClass(inputHandle)

    return False
# processjar.py 

def processJar(inputHandle):
    stream = inputHandle.getHandle()

    analytics=[]
    for a in State.analyticSet:
        name = a.get_name()
        if name in State.enabledAnalytics:
            analytics.append(a())

    State.currentJar.append(analytics)

    for a in analytics:
        if not a.scans_input_stream():
            continue
        stream = a.add_input_stream(stream)

    inputHandle.setStream(stream)

    checkZip(inputHandle)

    result = False
    for a in analytics:
        if a.identify(inputHandle):
            result = True

    State.currentJar.pop()
    return result
# zipinputstream.py 

#import sys
#import io
#import struct

class NonStreamable(Exception):
    pass

class ZipDecodeError(Exception):
    pass
#
# This allows processing a Zip file in streaming mode.  It's just utilizing
# the existing zipfile infrastucture.  Usage:
#
# inputHandle = open(filename, 'rb')
# zstream = ZipInputStream(inputHandle)
#
# for entry in zstream.nextFile():
#     filename = entry.name
#     x = entry.read(...)
#
class ZipInputStream:
    __handle = None
    __current = None
    __zinfo = None
    __data = None
    
    def __init__(self, filehandle):
        self.__handle = filehandle
        self.__current = None
        self.__data = None
        z = self.__zinfo = ZipFile.ZipInfo()
        z.orig_filename = None
        z.filename = None
        z.date_time = None
        z.compress_type = None
        z.comment = None
        z.extra = None
        z.create_system = None
        z.create_version = None
        z.extract_version = None
        z.reserved = None
        z.flag_bits = None
        z.volume = None
        z.internal_attr = None
        z.external_attr = None
        z.header_offset = None
        z.CRC = None
        z.compress_size = None
        z.file_size = None
        z._raw_time = None

    def readN(self, nbytes):
        res = None
        while nbytes > 0:
            d = self.__handle.read(nbytes)
            if d is None:
                break
            n = len(d)
            if n <= 0:
                break
            if res is None:
                res = d
            else:
                res = res + d
            nbytes = nbytes - n
        return res

    def nextFile(self):

        zinfo = self.__zinfo

        while True:

            if self.__current is not None:
                try:
                    while True:
                        d = self.__current.read(1024)
                        if d is None:
                            break
                        n = len(d)
                        if n <= 0:
                            break
                except:
                    pass

                if zinfo.flag_bits & 0x08 == 0x08:
                    d = self.readN(4)
                    if d == b'PK\x07\x08':
                        self.readN(12)
                    else:
                        self.readN(8)

            if self.__data is None:
                buffer = self.readN(ZipFile.sizeFileHeader)
            else:
                buffer = self.__data
                self.__data = None

            if buffer is None:
                break

            if len(buffer) != ZipFile.sizeFileHeader:
                raise AttributeError("Truncated file header")

            header = struct.unpack(ZipFile.structFileHeader, buffer)

            if header[ZipFile._FH_SIGNATURE] == ZipFile.stringEndArchive:
                break
            if header[ZipFile._FH_SIGNATURE] == ZipFile.stringCentralDir:
                break
            if header[ZipFile._FH_SIGNATURE] == ZipFile.stringEndArchive64Locator:
                break
            if header[ZipFile._FH_SIGNATURE] == ZipFile.stringEndArchive64:
                break

            if header[ZipFile._FH_SIGNATURE] != ZipFile.stringFileHeader:
                self.__data = buffer
                raise ZipDecodeError("Bad magic number for file header")

            fname = self.readN(header[ZipFile._FH_FILENAME_LENGTH])

            if header[ZipFile._FH_EXTRA_FIELD_LENGTH]:
                self.readN(header[ZipFile._FH_EXTRA_FIELD_LENGTH])

            if header[ZipFile._FH_GENERAL_PURPOSE_FLAG_BITS] & 0x800:
                fname_str = fname.decode("utf-8")
            else:
                fname_str = fname.decode("cp437")

            zinfo.compress_type = header[ZipFile._FH_COMPRESSION_METHOD]
            zinfo.compress_size = header[ZipFile._FH_COMPRESSED_SIZE]
            zinfo.file_size = header[ZipFile._FH_UNCOMPRESSED_SIZE]
            zinfo.extract_version = header[ZipFile._FH_EXTRACT_VERSION]
            zinfo.create_system = header[ZipFile._FH_EXTRACT_SYSTEM]
            zinfo.filename = fname_str
            zinfo.CRC = header[ZipFile._FH_CRC]
            zinfo.flag_bits = header[ZipFile._FH_GENERAL_PURPOSE_FLAG_BITS]
            if zinfo.file_size == 0 and zinfo.compress_size == 0 and (zinfo.flag_bits & 0x08 == 0x08):
                raise NonStreamable("Zip file can't be streamed at "+fname_str)

            self.__current = ZipFile.ZipExtFile(self.__handle,
                                          'r',
                                          zinfo)
            return self.__current

    def attemptResync(self):
        data = b''
        if self.__data is not None:
            data = self.__data
            self.__data = None
        if len(data) < ZipFile.sizeFileHeader:
            n = ZipFile.sizeFileHeader - len(data)
            b = self.readN(n)
            if b is None:
                return
            data = data + b

        while True:
            i = 0
            n = len(data)
            while i < n-1:
                if data[i] == 80 and data[i+1] == 75:
                    data = data[i:]
                    n = len(data)
                    if n < ZipFile.sizeFileHeader:
                        n = ZipFile.sizeFileHeader - n
                        b = self.readN(n)
                        if b is not None:
                            data = data + b
                    if data[2] == 3 and data[3] == 4:
                        self.__current = None
                        self.__data = data
                        return True
                i = i + 1
            n = ZipFile.sizeFileHeader
            if data[-1] == 'P':
                data = data[-1:]
                n = n - 1
            else:
                data = b''
            buf = self.readN(n)
            if buf is None:
                return False
            if len(buf) != n:
                return False
            data = data + buf
# ziphandler.py 

def checkZip(inputHandle):

    stream = inputHandle.getHandle()

    if inputHandle.isFile():
        with ZipFile.ZipFile(inputHandle.getName()) as z:
            for file in z.infolist():
                fn = file.filename
                with z.open(fn,'r') as h:
                    #fd = BytesIO(h.read())
                    #pin = Input(fn, getFileType(fn), fd)
                    pin = Input(fn, getFileType(fn), h)
                    inputHandle.addChild(pin)
                    try:
                        dispatchFile(pin)
                    except Exception as e:
                        errorOut("ZH2:"+str(e))
                        traceback.print_exc()
                    pin.clean()

    else:
        zin = ZipInputStream(stream)

        try:
            while True:
                try:
                    entry = zin.nextFile()
                    if entry is None:
                        break
                    fn = entry.name
                    if fn[-1] == '/':
                        continue
                    fin = Input(fn, getFileType(fn), entry)
                    inputHandle.addChild(fin)
                    try:
                        dispatchFile(fin)
                    except Exception as e:
                        errorOut("ZH1:"+str(e))
                    fin.clean()
                except ZipDecodeError as e:
                    if not zin.attemptResync():
                        break

        except NonStreamable as e:
            analytic = JarName()
            analytic.identify(inputHandle)
            
                            
        
# tarfile.py 


TAG_File = 48

def getString(data, offset, limit):
    for n in range(0,limit):
        d = data[offset+n:offset+n+1]
        b = struct.unpack('B', d)[0]
        if b == 0:
            end = offset+n
            return data[offset:end].decode()
    end = offset+limit
    return data[offset:end].decode()

def readN(file, n):
    res=None
    while n > 0:
        b = file.read(n)
        if len(b) == 0:
            break
        nb = len(b)
        if res is None:
            res = b
        else:
            res = res + b
        n = n - nb
    if res is None:
        return b''
    return res

class TarSparseUnsupported(Exception):
    pass

def checkTar(inputHandle):
    file = inputHandle.getHandle()
    status = False
    while True:
        data = readN(file,512);
        if len(data) != 512:
            break
        #if data[482] != 0:
        #raise TarSparseUnsupported(inputHandle.getName()+" contains sparse records which are unsupported.")
        d = data[156:157]
        type = struct.unpack('B',d)[0]
        if type != TAG_File:
            continue
        name = getString(data, 0, 100)
        ms = getString(data, 100, 8)
        ss = getString(data, 124, 12)
        size = int(ss, base=8)
        nblocks = int(size / 512)
        if nblocks * 512 != size:
            nblocks = nblocks + 1
        tar = TarFileReader(file, size, name=name)
        tin = Input(name, getFileType(name), tar)
        inputHandle.addChild(tin)
        if dispatchFile(tin):
            status = True
        tin.clean()
        tin.close()
    return status
# docker.py 

class Docker:

    def getContainerList(self):
        cmd = ("docker", "ps", "--format", "{{.ID}}")
        return self.runpipe(cmd)

    def getImageList(self):
        cmd = ("docker", "images", "--format", "{{.ID}}")
        return self.runpipe(cmd)

    def getVolumeList(self):
        cmd = ("docker", "volume", "ls", "--format", "{{.Name}}|{{.Mountpoint}}")
        return self.runpipe(cmd)

    def runpipe(self,cmd):
        p = subprocess.Popen(cmd, stdout=subprocess.PIPE)
        lines=[]
        if sys.version_info.major == 3:
            with TextIOWrapper(p.stdout, encoding="utf-8") as pin:
                for line in pin:
                    line = line.rstrip()
                    lines.append(line)
        else:
            for line in p.stdout:
                line = line.rstrip()
                lines.append(line)
        p.wait()
        return lines
                    
# version.py 

class Version:

    __version = None
    __analytics = set()
    __evidence = {}

    def __init__(self, version, analytic=None):
        self.__version = version
        self.__appname = None
        self.__notes = None
        if analytic is not None:
            self.__analytics.add(analytic)

    def getVersionID(self):
        return  self.__version

    def addEvidence(self,a,e):
        if a not in self.__evidence:
            self.__evidence[a] = []
        if e not in self.__evidence[a]:
            self.__evidence[a].append(e)

    def add_note(self, note):
        if self.__notes is None:
            self.__notes = []
        self.__notes.append(note)

    def add(self, v):
        for a in v.__analytics:
            self.__analytics.add(a)
        for a in v.__evidence:
            for e in v.__evidence[a]:
                self.addEvidence(a,e)

    def to_dict(self):
        res = {}

        res['version'] = self.__version;

        if len(self.__analytics) != 0:
            res['analytics'] = list(self.__analytics)

        if len(self.__evidence) != 0:
            eset = []
            for a in self.__evidence:
                ev = {}
                ev['analytic'] = a
                ev['evidence'] = self.__evidence[a]
                eset.append(ev)
            res['evidence'] = eset

        if self.__notes is not None:
            res['notes'] = self.__notes

        return res

    def toJSON(self):
        result = "{\"version\":\""+self.__version+"\""

        if len(self.__analytics) != 0:
            result = result + ",\"analytics\":["
            result = result + ",".join(["\""+x+"\"" for x in self.__analytics])
            result = result + "]"
        
        if len(self.__evidence) != 0:
            result = result + ",\"evidence\":["
            sep = ""
            for a in self.__evidence:
                result = result + sep
                sep = ","
                result = result + "{\"analytic\":\""+a+"\",\"evidence\":["
                result = result + ",".join(["\""+x+"\"" for x in self.__evidence[a]])
                result = result + "]}"
            result = result + "]"

        if self.__notes is not None:
            result = result + ",\"notes\":["
            sep = ""
            for note in self.__notes:
                result += sep
                sep = ","
                result += '"' + note + '"'
            result += ']'
            
        result = result + "}"
        return result
# cmdline.py 

def makeParser():
    parser = argparse.ArgumentParser()

    parser.add_argument("-e", "--enable", action='append', help="Enable specific analytic; use --list-analytics for names of analytics.")
    parser.add_argument('--report', action='store_true', help="Send JSON through report generator.")
    parser.add_argument("-v", "--verbose", action='count', help="Show information about what is going on.")
    parser.add_argument("-r","--running", action='store_true', help="Analyze running processes.")
    parser.add_argument("-s", "--search", action='store_true', help="Scan mounted file systems.")
    parser.add_argument("-F", "--file-system", action='append', help="Scan a specific file-system/directory.")
    parser.add_argument("--prune-fs", action='append', help="Do not allow file system scans to scan the specified file-system/directory.")
    parser.add_argument("--analytic-data", type=str, help="Specify alternate analytic data file to load.")
    parser.add_argument("-T", "--scan-tarfiles", action='store_true', help="Scan any tar files that are discovered.")
    parser.add_argument("-Z", "--scan-zipfiles", action='store_true', help="Scan any zip files that are discovered.")
    parser.add_argument("-D", "--scan-docker", action='append', help="Scan docker containers,images or volumes.")
    parser.add_argument("--system-packages", action='store_true', help="Use system package manager to quickly locate candidate files.")
    parser.add_argument("-H", "--hostname", type=str, help="Specify the hostname to use in the output record.")
    parser.add_argument("-a", "--list-applications", action='store_true', help="List the applications that are checked.")
    parser.add_argument("--list-analytics", action='store_true', help="List the available analytics for use with --enable.")
    parser.add_argument("--full", action='store_true', help="Give full description of each analytic.")
    parser.add_argument("--no-evidence",action='store_true', help="Don't include evidence field in version records.")
    parser.add_argument('--ansible-managed', action='store_true', help="Internal option for ansible")
    parser.add_argument('--check-tables-ready', action='store_true', help="Internal option for ansible")
    parser.add_argument("file", nargs='*')
    return parser

# main.py 

def printError(s):
    sys.stderr.write(s+"\n")


def main(args, on_success, on_error, errormsgs=None):
    global errorOut
    if errormsgs is None:
        errorOut = printError
    else:
        errorOut = on_error

    if args['analytic_data'] is not None:
        adfn = args['analytic_data']
        try:
            with open(adfn, "r") as adf:
                ConfigurationData.analytic_data = json.load(adf)
        except Exception as e:
            errorOut(adfn+": "+str(e))
            exit(1)
    elif ConfigurationData.analytic_data is None:
        if args['ansible_managed']:
            errorOut("This jaudit can not be used with Ansible as it has no built-in analytic data.")
        else:
            errorOut("No analytic data included in script. Must specify using --analytic-data option.")
        exit(1)

    #
    # To add new analytics, implement the JarIdentifier "abstract"
    # class as defined in 'jar_identifier.py', then add the python
    # source file name to ../cf/template.cf
    #
    # The analytics will be filtered based on the result of the
    # supported() method.  Those that return False, will be dropped.
    #
    # The analytic with the highest priority (priority() class method)
    # will be the default analytic.
    #
    
    all_analytics = sorted(getplugins(JarIdentifier), key=lambda a: a.priority(), reverse=True)

    State.analyticSet = list(filter(lambda a : a.supported(), all_analytics))

    if len(State.analyticSet) == 0:
        errorOut("No analytics are usable in this build.")
        return 

    commandLineError = False

    if args['list_applications']:
        apps = Configuration.get_analytic_data('enabled_apps')
        print(",".join(apps))
        return
              

    if args['list_analytics']:
        print("")
        default='*'
        for a in State.analyticSet:
            name = a.get_name()
            name = name + default
            default = ""
            if not args['full']:
                print(name)
                continue
            desc = a.get_description()
            s = ' ' * (19 - len(name))
            line=name+s
            output=""
            words = desc.split(' ')
            count = 0
            for w in words:
                n = len(w)
                if len(line)+n + 1 > 72:
                    output = output + line + "\n"
                    count = count + 1
                    line = ' ' * 19
                line = line + ' ' + w
            if len(line) > 19:
                output = output + line
                count = count + 1
            print(output)
            if count > 1:
                print("")
        print("\n  * denotes the default analytic\n")
        return

    #
    # Validate any values passed to the '-D'/'--scan-docker' option
    # Valid values are 'containers','images' and 'volumes'
    #
    dockerTests = set()
    if args['scan_docker'] is not None:
        for a in args['scan_docker']:
            if a is None:
                continue
            for v in a.split(','):
                if v in ('containers','images','volumes'):
                    dockerTests.add(v)
                else:
                    errorOut("Unrecognized value for --scan-docker: "+v)
                    commandLineError = True

    #
    # Get the names of all of the analytics
    #
    for a in State.analyticSet:
        State.analyticNames.add(a.get_name())

    #
    # Validate the enabled analytics and add them to the enabled
    # list.  If nothing was enabled, use the first analytic in the
    # set as the enabled analytic.
    #
    if args['enable'] is not None:
        for aset in args['enable']:
            if aset is None:
                continue
            for a in aset.split(','):
                if a not in State.analyticNames:
                    errorOut("Unrecognized analytic name for --enable: "+a)
                    commandLineError = True
                else:
                    State.enabledAnalytics.add(a)
    else:
        State.enabledAnalytics.add(State.analyticSet[0].get_name())

    #
    # Initialize the enabled analytics.  The initialization step is only
    # called once. It allows the analytic to do any upfront processing.
    #
    failed = set()
    for a in State.analyticSet:
        name = a.get_name()
        if name in State.enabledAnalytics:
            if not a.initialize():
                failed.add(name)

    for name in failed:
        State.enabledAnalytics.remove(name)
        errorOut("Unable to initialize analytic "+name+".")


    #
    # If any errors occurred while processing the command line, then
    # call the error processor.
    #
    if commandLineError:
        on_error(None)
        sys.exit(1)    # on_error *should* exit... but just in case

    #
    # Intialize the global state
    #
    State.verbose = args['verbose']
    if State.verbose is None:
        State.verbose = 0
    State.addEvidence = True
    if args['no_evidence'] is True:
        State.addEvidence = False

    State.scanTarFiles = args['scan_tarfiles']
    State.scanZipFiles = args['scan_zipfiles']
    
    files=args['file']

    #
    # Get a hostname and create an outer Input using the hostname
    # as the name of the Input.  All other inputs will be attached
    # to this Input.
    #
    if args['hostname'] is not None:
        hostname = args['hostname']
    else:
        hostname=gethostname()

    host=Input(hostname,"host",None)
    host.setComment("python"+sys.version[0])

    scannedFiles = set()

    #------------------------------------------------------------------------
    #
    # Scan any files provided on the command line
    #
    for fn in files:
        try:
            rn = os.path.realpath(fn)
            if rn in scannedFiles:
                continue
            inputHandle = FileInput(fn, rn, getFileType(fn))
            host.addChild(inputHandle)
            dispatchFile(inputHandle, fromUser=True)
            scannedFiles.add(rn)
            inputHandle.clean()
            inputHandle.close()
        except Exception as e:
            traceback.print_exc()
            errorOut("main1:"+str(e))

    if args['system_packages']:
        patterns=[]
        patterns.append('.*\\.jar$')
        patterns.append('.*\\.class$')
        if State.scanZipFiles:
            patterns.append(".*\\.zip$")
        if State.scanTarFiles:
            patterns.append(".*\\.tar$")
            patterns.append(".*\\.tgz$")
            patterns.append(".*\\.tar.gz$")

        pkgfiles, count = SysPackages.get_file_names(patterns)

        if pkgfiles is None:
            errorOut("System package query not supported on this platform")
        else:
            if State.verbose > 1:
                errorOut("Checking " + str(count) + " files found from system package information.")
            for ptype in pkgfiles:
                for pkg in pkgfiles[ptype]:
                    pin = Input(pkg, ptype, None)
                    host.addChild(pin)
                    for file in pkgfiles[ptype][pkg]:
                        if file in scannedFiles:
                            continue
                        if State.verbose:
                            errorOut("Scanning "+file+" from package "+pkg)
                            try:
                                inputHandle = FileInput(file, file, getFileType(file))
                                pin.addChild(inputHandle)
                                scannedFiles.add(file)
                                dispatchFile(inputHandle)
                                inputHandle.clean()
                                inputHandle.close()
                            except Exception as e:
                                errorOut("main3:"+str(e))
                    pin.clean()
        

    procInfo = ProcInfo()
    #
    # If they requested scanning running processes, check the files
    # they have open
    #
    if args['running']:
        skipargs=['-cp','-classpath']
        canGetOpenFiles = True
        for pid in procInfo.getPids():
            name = procInfo.getExecutable(pid)
            if name is None:
                continue
            if name.endswith("java") or name.endswith("java.exe"):
                id = name+"["+str(pid)+"]"
                cmdargv = procInfo.getCommandLine(pid)
                cmdline = ' '.join(cmdargv)
                progname = None
                ndx = 1
                isSameNS = procInfo.isSameNS(pid)
                cmdfiles = []
                searchPath=set()
                while ndx < len(cmdargv):
                    if len(cmdargv[ndx]) == 0:
                        ndx = ndx + 1
                        continue
                    if cmdargv[ndx] == '-jar':
                        if ndx+1 < len(cmdargv):
                            progname = cmdargv[ndx+1]
                            cmdfiles.append(progname)
                            break
                    if cmdargv[ndx] in skipargs:
                        if cmdargv[ndx] == '-cp' or cmdargv[ndx] == '-classpath':
                            if ndx+1 < len(cmdargv):
                                cp = cmdargv[ndx+1]
                                sc = ':'
                                if os.sys.platform[0:5] == 'win32':
                                    sc=';'
                                for p in cp.split(sc):
                                    if isJarFile(p):
                                        cmdfiles.append(p)
                                    elif os.path.isdir(p):
                                        searchPath.add(p)
                        ndx = ndx + 1
                    elif progname is None and cmdargv[ndx][0] != '-':
                        progname = cmdargv[ndx]
                    ndx = ndx + 1

                if progname is not None:
                    id = progname+"["+str(pid)+"]"

                pin = Input(id,"process", None)
                pin.setComment(cmdline)
                host.addChild(pin)
                if State.verbose > 0:
                    errorOut("Scanning process "+id)

                filelist = procInfo.getOpenFiles(pid)

                if filelist is None:
                    if canGetOpenFiles:
                        errorOut("openfiles command for local files is not enabled.\n\tUse: openfiles /local on\nto enable.")
                        canGetOpenFiles = False
                        continue
                scanned = set()
                for fsname,dname in filelist:
                    if os.path.isdir(fsname):
                        continue
                    if dname in scanned:
                        continue
                    scan=False
                    if isJarFile(dname) or dname.endswith(".class"):
                        scan=True
                    elif State.scanZipFiles and dname.endswith(".zip"):
                        scan=True
                    elif State.scanTarFiles and (dname.endswith(".tar") or dname.endswith(".tgz") or dname.endswith(".tar.gz")):
                        scan=True

                    if not scan:
                        continue

                    scanned.add(dname)
                    
                    try:
                        inputHandle = FileInput(fsname, dname, getFileType(dname))
                        pin.addChild(inputHandle)
                        dispatchFile(inputHandle)
                        if not isSameNS:
                            inputHandle.setName("NS(" + dname + ")")
                        inputHandle.clean()
                        inputHandle.close()
                    except Exception as e:
                        errorOut("main2:"+str(e))

                for name in cmdfiles:
                    if not os.path.exists(name):
                        for dir in searchPath:
                            if os.path.exists(dir + '/' + name):
                                name = dir + '/' + name
                                break
                    if name in scanned:
                        continue
                    scanned.add(name)
                    try:
                        inputHandle = FileInput(name, name, getFileType(name))
                        pin.addChild(inputHandle)
                        dispatchFile(inputHandle)
                        if not isSameNS:
                            inputHandle.setName("NS(" + name + ")")
                        inputHandle.clean()
                        inputHandle.close()
                    except Exception as e:
                        errorOut("main3:"+str(e))

                pin.clean()
    #
    # Create a set of file types that we consider 'local'
    #
    localFSTypes = set()
    for fs in ('ext2','ext3','ext4','vfat','fat32','ecryptfs','btrfs','xfs','jfs', 'jfs2', 'iso9660','reiserfs', 'apfs', 'zfs', 'ufs', 'removable-disk','fixed-local-disk','cdrom'):
        localFSTypes.add(fs)

    #
    # Generate a set of NON-local file systems by inverting the local
    # file system types
    #
    toSkip = set()
    fs = FileSystem()
    for fs in fs.getFileSystems(localFSTypes,invert=True):
        toSkip.add(fs)

    if args['prune_fs'] is not None:
        for pfs in args['prune_fs']:
            if pfs is None:
                continue
            for fs in pfs.split(','):
                toSkip.add(fs)

    #
    # Scan docker containers
    #
    if 'containers' in dockerTests:
        dk = Docker()
        for container in dk.getContainerList():
            p = subprocess.Popen(("docker", "export", container), stdout=subprocess.PIPE)
            inputHandle = Input(container, "docker-container", p.stdout)
            host.addChild(inputHandle)
            if State.verbose > 0:
                errorOut("Scanning docker container "+container)
            try:
                checkTar(inputHandle)
            except Exception as e:
                errorOut("main3:"+str(e))
            inputHandle.clean()
            p.wait()
    #
    # Scan docker images
    #
    if 'images' in dockerTests:
        dk = Docker()
        for image in dk.getImageList():
            p = subprocess.Popen(("docker", "save", image), stdout=subprocess.PIPE)
            inputHandle = Input(image, "docker-image", p.stdout)
            host.addChild(inputHandle)
            if State.verbose > 0:
                errorOut("Scanning docker image "+image)
            try:
                checkTar(inputHandle)
            except Exception as e:
                errorOut("main4:"+str(e))

            inputHandle.clean()
            p.wait()

    #
    # Scan docker volumes
    #
    if 'volumes' in dockerTests:
        dk = Docker()
        for volloc in dk.getVolumeList():
            name,location = volloc.split("|")
            inputHandle = Input(name, "docker-volume", None)
            host.addChild(inputHandle)
            if State.verbose > 0:
                errorOut("Scanning docker volume "+name)
            try:
                scanfs(location, toSkip, inputHandle)
            except Exception as e:
                errorOut("main5:"+str(e))
            inputHandle.clean()

    #
    # Scan any directories the user specified to scan
    #
    if args['file_system'] is not None:
        for a in args['file_system']:
            f = a.split(',')
            for fs in f:
                rfs = os.path.realpath(fs)
                if not os.path.isdir(rfs):
                    errorOut(rfs+" is not a directory; scanning as file")
                    try:
                        inputHandle = FileInput(rfs, rfs, getFileType(rfs))
                        host.addChild(inputHandle)
                        dispatchFile(inputHandle)
                        inputHandle.clean()
                        inputHandle.close()
                    except Exception as e:
                        errorOut("main6:"+str(e))

                    continue
                inputHandle = Input(rfs,"directory", None)
                host.addChild(inputHandle)
                if State.verbose > 0:
                    errorOut("Scanning directory "+rfs)
                scanfs(fs, toSkip, inputHandle)
                inputHandle.clean()
                # Avoid scanning it twice
                toSkip.add(rfs)

    #
    # If user wants to scan all mounted local file systems, then scan them
    #
    if args['search']:
        fs = FileSystem()
        toScan = list(fs.getFileSystems(localFSTypes))
        for fs in toScan:
            inputHandle = Input(fs,"file-system",None)
            host.addChild(inputHandle)
            if State.verbose > 0:
                errorOut("Scanning file system "+fs)
            skips = set()
            for s in toSkip:
                skips.add(s)
            for f in toScan:
                if f != fs:
                    skips.add(f)
            scanfs(fs, skips, inputHandle)
            inputHandle.clean()


    if len(State.looseClasses) != 0:
        cg = ClassGrouping()
        cf = cg.groupClassFiles(State.looseClasses)
        for dir in cf:
            inputHandle = Input(os.path.realpath(dir), "directory",None)
            inputHandle.setComment("Unpackaged class files")
            host.addChild(inputHandle)
            cg.classPkgVersion(cf[dir], inputHandle)
            inputHandle.clean()

    #
    # All done... everything should be in a nested data structure inside
    # of the 'host' Input... convert that to JSON and print it...
    #
    on_success(host.to_dict())

# jarversions.py 

def json_writer(record):
    print(json.dumps(record,separators=(',',':')))

def report_writer(record):
    tr = TextReport()
    tr.convert(record)
    print(tr.get())


def errorOccurred(s):
    if s is not None:
        sys.stderr.write(s+"\n")
    sys.exit(1)

if __name__ == '__main__':

    p = makeParser()
    args = vars(p.parse_args())

    if args['ansible_managed']:
        if args['report']:
            errorOccurred("Can't use --report when used with Ansible.")

    if args['check_tables_ready']:
        if ConfigurationData.analytic_data is None:
            exit(1)
        exit(0)

    writer = json_writer
    if args['report']:
        writer = report_writer
            
    try:
        main(args, writer, errorOccurred)
    except KeyboardInterrupt:
        sys.exit(1)

